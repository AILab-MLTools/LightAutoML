{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f52db5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/trinity/home/r.zagidullin/miniconda3/envs/new_lama_venv/bin/python3.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Queue\n",
    "from itertools import product\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4f445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a7f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of GPUs available\n",
    "NAME = 'LAMA_PRESET_PF_SMALL_TO_OLD'\n",
    "PREFIX = 'runs'\n",
    "#GPU_PARTS = [[0, 1], [2, 3]]\n",
    "gpu = [0,1]\n",
    "RUNNER_PATH = 'runners/lama_pf.py'\n",
    "CONFIG_PATH = 'runners/lama_pf.yml'\n",
    "DEBUG = True\n",
    "\n",
    "TIMEOUT = 720\n",
    "HARD_TIMEOUT = 720\n",
    "NTHREADS = 4\n",
    "SEED = 42\n",
    "\n",
    "# benchmark_params\n",
    "data_path = '../../data/old_presets/data'\n",
    "benchmark_path = '../../data/old_presets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc167e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put indices in queue\n",
    "#q = Queue(maxsize=len(GPU_PARTS))\n",
    "#for i in GPU_PARTS:\n",
    "#    q.put(i)\n",
    "\n",
    "data_info = joblib.load(os.path.join(benchmark_path, 'data_info.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece99dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covertype': {'path': 'openml/covertype.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'multiclass',\n",
       "  'class_map': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6},\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'albert': {'path': 'openml/albert.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'binary',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'higgs': {'path': 'openml/higgs.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'binary',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'guillermo': {'path': 'openml/guillermo.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'binary',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'bank-marketing': {'path': 'openml/bank-marketing.csv',\n",
       "  'target': 'Class',\n",
       "  'task_type': 'binary',\n",
       "  'class_map': {1: 0, 2: 1},\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'numerai28.6': {'path': 'openml/numerai28.6.csv',\n",
       "  'target': 'attribute_21',\n",
       "  'task_type': 'binary',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'volkert': {'path': 'openml/volkert.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'multiclass',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'adult': {'path': 'openml/adult.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'binary',\n",
       "  'class_map': {' <=50K': 0, ' >50K': 1},\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'MiniBooNE': {'path': 'openml/MiniBooNE.csv',\n",
       "  'target': 'signal',\n",
       "  'task_type': 'binary',\n",
       "  'class_map': {False: 0, True: 1},\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'dilbert': {'path': 'openml/dilbert.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'multiclass',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'riccardo': {'path': 'openml/riccardo.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'binary',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'shuttle': {'path': 'openml/shuttle.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'multiclass',\n",
       "  'class_map': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6},\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'KDDCup09_appetency': {'path': 'openml/KDDCup09_appetency.csv',\n",
       "  'target': 'APPETENCY',\n",
       "  'task_type': 'binary',\n",
       "  'class_map': {-1: 0, 1: 1},\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'Fashion-MNIST': {'path': 'openml/Fashion-MNIST.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'multiclass',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'connect-4': {'path': 'openml/connect-4.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'multiclass',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'airlines': {'path': 'openml/airlines.csv',\n",
       "  'target': 'Delay',\n",
       "  'task_type': 'binary',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'jannis': {'path': 'openml/jannis.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'multiclass',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'nomao': {'path': 'openml/nomao.csv',\n",
       "  'target': 'Class',\n",
       "  'task_type': 'binary',\n",
       "  'class_map': {1: 0, 2: 1},\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'Amazon_employee_access': {'path': 'openml/Amazon_employee_access.csv',\n",
       "  'target': 'target',\n",
       "  'task_type': 'binary',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'robert': {'path': 'openml/robert.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'multiclass',\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'aps_failure': {'path': 'openml/aps_failure.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'binary',\n",
       "  'class_map': {'neg': 0, 'pos': 1},\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'jungle_chess_2pcs_raw_endgame_complete': {'path': 'openml/jungle_chess_2pcs_raw_endgame_complete.csv',\n",
       "  'target': 'class',\n",
       "  'task_type': 'multiclass',\n",
       "  'class_map': {'w': 0, 'b': 1, 'd': 2},\n",
       "  'read_csv_params': {'na_values': '?'}},\n",
       " 'ashrae-energy-prediction': {'path': 'ashrae-energy-prediction/processed_train.csv',\n",
       "  'target': 'meter_reading',\n",
       "  'task_type': 'reg'},\n",
       " 'ieee-fraud-detection': {'path': 'ieee-fraud-detection/processed_train.csv',\n",
       "  'target': 'isFraud',\n",
       "  'task_type': 'binary'},\n",
       " 'bnp-paribas-cardif-claims-management': {'path': 'bnp-paribas-cardif-claims-management/train.csv',\n",
       "  'target': 'target',\n",
       "  'task_type': 'binary',\n",
       "  'drop': ['ID']},\n",
       " 'porto-seguro-safe-driver-prediction': {'path': 'porto-seguro-safe-driver-prediction/train.csv',\n",
       "  'target': 'target',\n",
       "  'task_type': 'binary',\n",
       "  'drop': ['id']},\n",
       " 'springleaf-marketing-response': {'path': 'springleaf-marketing-response/train.csv',\n",
       "  'target': 'target',\n",
       "  'task_type': 'binary',\n",
       "  'drop': ['ID']},\n",
       " 'talkingdata-adtracking-fraud-detection': {'path': 'talkingdata-adtracking-fraud-detection/train.csv',\n",
       "  'target': 'is_attributed',\n",
       "  'task_type': 'binary',\n",
       "  'drop': ['attributed_time']}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5ed0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task(benchmark_path, data_path, dataset, fold, rewrite=False):\n",
    "    #gpu = q.get()\n",
    "    \n",
    "    run_name = dataset + '_' + str(fold)\n",
    "    \n",
    "    print('Processing', run_name)\n",
    "    \n",
    "    benchmark_path = os.path.abspath(benchmark_path)\n",
    "    data_path = os.path.abspath(data_path)\n",
    "    output = os.path.join(benchmark_path, PREFIX, NAME, dataset, 'fold_{0}'.format(fold))\n",
    "    \n",
    "#     sleep(5)\n",
    "    \n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    \n",
    "    success_flg = os.path.join(output, 'SUCCESS')\n",
    "    \n",
    "    #if os.path.exists(success_flg) and not rewrite:\n",
    "    #    q.put(gpu)\n",
    "    #    return \n",
    "    \n",
    "    # clean folder\n",
    "    for f in (x for x in os.listdir(output) if not x.startswith('.')):\n",
    "        os.remove(os.path.join(output, f))\n",
    "    \n",
    "    # TRAIN\n",
    "    try:\n",
    "        \n",
    "        script = \"source /trinity/home/r.zagidullin/miniconda3/bin/activate; conda activate new_lama_venv; which python; \"\n",
    "        \n",
    "        log = subprocess.check_output(script + ' '.join([\n",
    "            'python', \n",
    "            RUNNER_PATH,\n",
    "            '-b', benchmark_path,\n",
    "            '-p', data_path,\n",
    "            '-k', dataset,\n",
    "            '-f', str(fold), \n",
    "            '-n', str(NTHREADS),\n",
    "            '-s', str(SEED),\n",
    "            '-d', ','.join(map(str, gpu)),\n",
    "            '-c', os.path.abspath(CONFIG_PATH),\n",
    "            '-t', str(TIMEOUT),\n",
    "            '-o', output\n",
    "\n",
    "        ]), timeout=HARD_TIMEOUT, shell=True, stderr=subprocess.STDOUT, executable='/bin/bash').decode()\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(log)\n",
    "        \n",
    "        with open(success_flg, 'w') as f:\n",
    "            pass\n",
    "        \n",
    "        with open(os.path.join(output, 'train_log.txt'), 'w') as f:\n",
    "            f.write(log)\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "\n",
    "        print(e.output.decode())\n",
    "        \n",
    "        with open(os.path.join(output, 'ERROR'), 'w') as f:\n",
    "            pass\n",
    "        \n",
    "        with open(os.path.join(output, 'train_log.txt'), 'w') as f:\n",
    "            f.write(e.output.decode())\n",
    "                  \n",
    "    except subprocess.TimeoutExpired:\n",
    "        \n",
    "        \n",
    "        with open(os.path.join(output, 'TIMEOUT'), 'w') as f:\n",
    "            pass\n",
    "        \n",
    "        print('HARD TIMEOUT!')\n",
    "        \n",
    "    #q.put(gpu)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# def run_task(benchmark_path, data_path, dataset, fold, rewrite=False):\n",
    "    \n",
    "#     gpu = q.get()\n",
    "#     _run_task(benchmark_path, data_path, dataset, fold, gpu, rewrite)\n",
    "#     q.put(gpu)\n",
    "    \n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9baeb493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing airlines_4\n",
      "/trinity/home/r.zagidullin/miniconda3/envs/new_lama_venv/bin/python\n",
      "Train dataset airlines, fold 4\n",
      "  Airline  Flight AirportFrom AirportTo  DayOfWeek  Time  Length  Delay\n",
      "0      CO     269         SFO       IAH          3    15     205      1\n",
      "1      US    1558         PHX       CLT          3    15     222      1\n",
      "2      AA    2400         LAX       DFW          3    20     165      1\n",
      "3      AA    2466         SFO       DFW          3    20     195      1\n",
      "4      AS     108         ANC       SEA          3    30     202      0\n",
      "[13:35:41] Stdout logging level is DEBUG.\n",
      "[13:35:41] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[13:35:41] Task: binary\n",
      "\n",
      "[13:35:41] Start automl preset with listed constraints:\n",
      "[13:35:41] - time: 720.00 seconds\n",
      "[13:35:41] - CPU: 4 cores\n",
      "[13:35:41] - memory: 16 GB\n",
      "\n",
      "[13:35:41] Train data shape: (431507, 8)\n",
      "Feats was rejected during automatic roles guess: []\n",
      "[13:35:42] Layer \u001b[1m1\u001b[0m train process start. Time left 718.34 secs\n",
      "[13:35:44] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[13:35:44] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (par) =====\n",
      "5.00120222568512e-06 getting data\n",
      "[13:35:44] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (par) =====\n",
      "4.541128873825073e-06 getting data\n",
      "5.678778272122145 transfering model\n",
      "[13:35:54] Linear model: C = 1e-05 score = 0.7038885354995728\n",
      "[13:35:55] Linear model: C = 5e-05 score = 0.70604407787323\n",
      "[13:35:55] Linear model: C = 0.0001 score = 0.7066037654876709\n",
      "[13:35:56] Linear model: C = 0.0005 score = 0.7071499824523926\n",
      "[13:35:56] Linear model: C = 0.001 score = 0.7071498036384583\n",
      "[13:35:58] Linear model: C = 0.005 score = 0.7074733972549438\n",
      "[13:35:58] Linear model: C = 0.01 score = 0.7074733972549438\n",
      "[13:35:58] Linear model: C = 0.05 score = 0.7074733972549438\n",
      "8.61415584385395 fit data\n",
      "0.00296834297478199 predict data\n",
      "14.354192340746522 transfering model\n",
      "[13:36:04] Linear model: C = 1e-05 score = 0.70626300573349\n",
      "[13:36:05] Linear model: C = 5e-05 score = 0.7080655097961426\n",
      "[13:36:06] Linear model: C = 0.0001 score = 0.7085422277450562\n",
      "[13:36:07] Linear model: C = 0.0005 score = 0.7090892791748047\n",
      "[13:36:07] Linear model: C = 0.001 score = 0.7090892791748047\n",
      "[13:36:08] Linear model: C = 0.005 score = 0.7092434167861938\n",
      "[13:36:08] Linear model: C = 0.01 score = 0.7092434167861938\n",
      "[13:36:08] Linear model: C = 0.05 score = 0.7092434167861938\n",
      "10.059899099171162 fit data\n",
      "0.003405272960662842 predict data\n",
      "[13:36:08] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (par) =====\n",
      "4.719942808151245e-06 getting data\n",
      "[13:36:08] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (par) =====\n",
      "4.297122359275818e-06 getting data\n",
      "0.003278784453868866 transfering model\n",
      "0.011556986719369888 transfering model\n",
      "[13:36:09] Linear model: C = 1e-05 score = 0.7055079936981201\n",
      "[13:36:09] Linear model: C = 1e-05 score = 0.7068615555763245\n",
      "[13:36:10] Linear model: C = 5e-05 score = 0.7090438604354858\n",
      "[13:36:10] Linear model: C = 5e-05 score = 0.7072101831436157\n",
      "[13:36:11] Linear model: C = 0.0001 score = 0.7095293998718262\n",
      "[13:36:12] Linear model: C = 0.0001 score = 0.7077577114105225\n",
      "[13:36:13] Linear model: C = 0.0005 score = 0.7100396156311035\n",
      "[13:36:13] Linear model: C = 0.001 score = 0.7100396156311035\n",
      "[13:36:13] Linear model: C = 0.0005 score = 0.7081624269485474\n",
      "[13:36:13] Linear model: C = 0.001 score = 0.7081624269485474\n",
      "[13:36:13] Linear model: C = 0.005 score = 0.7100396156311035\n",
      "4.882382428273559 fit data\n",
      "0.003656787797808647 predict data\n",
      "[13:36:13] Linear model: C = 0.005 score = 0.7081620693206787\n",
      "4.964866429567337 fit data\n",
      "0.002951785922050476 predict data\n",
      "[13:36:13] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (par) =====\n",
      "5.554407835006714e-06 getting data\n",
      "0.000706743448972702 transfering model\n",
      "[13:36:14] Linear model: C = 1e-05 score = 0.7050706744194031\n",
      "[13:36:14] Linear model: C = 5e-05 score = 0.7072842121124268\n",
      "[13:36:15] Linear model: C = 0.0001 score = 0.7078288197517395\n",
      "[13:36:16] Linear model: C = 0.0005 score = 0.7084041833877563\n",
      "[13:36:16] Linear model: C = 0.001 score = 0.7084044814109802\n",
      "[13:36:18] Linear model: C = 0.005 score = 0.7086870670318604\n",
      "[13:36:18] Linear model: C = 0.01 score = 0.7086870670318604\n",
      "[13:36:18] Linear model: C = 0.05 score = 0.7086870670318604\n",
      "4.5851577129215 fit data\n",
      "0.0027908887714147568 predict data\n",
      "[13:36:18] Time history [24.48994278907776, 4.9869279861450195, 4.596652269363403]. Time left 87.9705762283222\n",
      "[13:36:18] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[13:36:18] Time left 683.02 secs\n",
      "\n",
      "[13:36:18] Start fitting Selector_CatBoost_gpu ...\n",
      "[13:36:18] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mSelector_CatBoost_gpu\u001b[0m (par) =====\n",
      "[13:36:50] Time history [32.43779754638672]. Time left 69.32317392031351\n",
      "[13:36:50] Selector_CatBoost_gpu fitting and predicting completed\n",
      "[13:36:52] Start fitting Lvl_0_Pipe_1_Mod_0_CatBoost_gpu ...\n",
      "[13:36:52] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[13:36:52] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[13:39:42] Time limit exceeded after calculating fold(s) [0 1]\n",
      "[13:39:42] Time history [32.43779754638672, 170.06033873558044]. Time left -54.861735248565665\n",
      "[13:39:43] Lvl_0_Pipe_1_Mod_0_CatBoost_gpu fitting and predicting completed\n",
      "[13:39:43] Start fitting Lvl_0_Pipe_1_Mod_1_XGB ...\n",
      "[13:39:43] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[13:39:43] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "21.275884553790092 xgb single fold time\n",
      "24.68414916470647 xgb single fold time\n",
      "[13:40:07] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[13:40:07] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "21.155683228746057 xgb single fold time\n",
      "27.088481478393078 xgb single fold time\n",
      "[13:40:35] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "12.995848750695586 xgb single fold time\n",
      "[13:40:48] Time history [24.786256551742554, 27.159401893615723, 13.032193899154663]. Time left 341.1707487106323\n",
      "[13:40:48] Lvl_0_Pipe_1_Mod_1_XGB fitting and predicting completed\n",
      "[13:40:48] Time left 413.14 secs\n",
      "\n",
      "[13:40:48] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[13:40:48] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[13:40:48] Blending: Optimization starts with equal weights and score 0.7202397584915161\n",
      "[13:40:48] Blending, iter 0: score = 0.7207834720611572, weights = [0.34733674 0.0733627  0.5793005 ]\n",
      "[13:40:49] Blending, iter 1: score = 0.7207841873168945, weights = [0.35179332 0.06147327 0.5867334 ]\n",
      "[13:40:50] Blending, iter 2: score = 0.7207841873168945, weights = [0.35179332 0.06147327 0.5867334 ]\n",
      "[13:40:50] No score update. Terminated\n",
      "[13:40:50] \u001b[1mAutoml preset training completed in 308.84 seconds\u001b[0m\n",
      "\n",
      "[13:40:50] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.35179 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.06147 * (2 averaged models Lvl_0_Pipe_1_Mod_0_CatBoost_gpu) +\n",
      "\t 0.58673 * (5 averaged models Lvl_0_Pipe_1_Mod_1_XGB) \n",
      "\n",
      "/bin/bash: line 1: 2274880 Killed                  python runners/lama_pf.py -b /beegfs/home/r.zagidullin/LAMA/LightAutoML/data/old_presets -p /beegfs/home/r.zagidullin/LAMA/LightAutoML/data/old_presets/data -k airlines -f 4 -n 4 -s 42 -d 0,1 -c /beegfs/home/r.zagidullin/LAMA/LightAutoML/examples/old/runners/lama_pf.yml -t 720 -o /beegfs/home/r.zagidullin/LAMA/LightAutoML/data/old_presets/runs/LAMA_PRESET_PF_SMALL_TO_OLD/airlines/fold_4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_task(benchmark_path, data_path, 'airlines', 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07eeac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ieee-fraud-detection_4\n",
      "/trinity/home/r.zagidullin/miniconda3/envs/new_lama_venv/bin/python\n",
      "/bin/bash: line 1: 2277179 Killed                  python runners/lama_pf.py -b /beegfs/home/r.zagidullin/LAMA/LightAutoML/data/old_presets -p /beegfs/home/r.zagidullin/LAMA/LightAutoML/data/old_presets/data -k ieee-fraud-detection -f 4 -n 4 -s 42 -d 0,1 -c /beegfs/home/r.zagidullin/LAMA/LightAutoML/examples/old/runners/lama_pf.yml -t 720 -o /beegfs/home/r.zagidullin/LAMA/LightAutoML/data/old_presets/runs/LAMA_PRESET_PF_SMALL_TO_OLD/ieee-fraud-detection/fold_4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_task(benchmark_path, data_path, 'ieee-fraud-detection', 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel(n_jobs=len(GPU_PARTS), backend=\"threading\")(\n",
    "#     delayed(run_task)(benchmark_path, d, f, False) for (d, s) in product(data_info, range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1c430",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing covertype_0\n",
      "Processing albert_0\n",
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset albert, fold 0\n",
      "   class    V1    V2    V3    V4       V5  ...  V73  V74   V75  V76   V77   V78\n",
      "0      1   2.0   0.0   9.0   5.0      0.0  ...   43   43   1.0  103   870   662\n",
      "1      1   NaN  -1.0   NaN   NaN   6736.0  ...   43   43   6.0  188  8016  1887\n",
      "2      0   1.0   0.0  11.0   4.0     20.0  ...   43   43   2.0   57  4740  1035\n",
      "3      0  10.0   0.0  33.0  37.0      1.0  ...  103   43  19.0  103  4741  1512\n",
      "4      0   NaN  24.0   5.0   9.0  13632.0  ...   43   43   1.0   43  5454  1138\n",
      "\n",
      "[5 rows x 79 columns]\n",
      "setting gpuids with all\n",
      "[14:12:23] Stdout logging level is DEBUG.\n",
      "[14:12:23] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:12:23] Task: binary\n",
      "\n",
      "[14:12:23] Start automl preset with listed constraints:\n",
      "[14:12:23] - time: 720.00 seconds\n",
      "[14:12:23] - CPU: 4 cores\n",
      "[14:12:23] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 4\n",
      "[14:12:39] Train data shape: (382716, 79)\n",
      "daskcudf reader: 5.763358375057578\n",
      "hybrid reader: 21.825827475637197\n",
      "[14:12:45] Layer \u001b[1m1\u001b[0m train process start. Time left 698.17 secs\n",
      "[14:14:19] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[14:14:29] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "pandas reader: 1.0041854539886117\n",
      "pandas reader: 0.9846516530960798\n",
      "pandas reader: 0.8871147036552429\n",
      "pandas reader: 5.207645785063505\n",
      "cudf reader: 10.947080022655427\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "cudf reader: 11.05242379475385\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "1.004151999950409e-05 getting data\n",
      "3.612733032554388 transfering model\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "50.693204804323614 fit data\n",
      "0.03888832684606314 predict data\n",
      "[14:15:34] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "1.1761672794818878e-05 getting data\n",
      "0.0020801126956939697 transfering model\n",
      "24.978124421089888 fit data\n",
      "0.03976061474531889 predict data\n",
      "[14:16:10] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "1.159869134426117e-05 getting data\n",
      "0.002829411067068577 transfering model\n",
      "25.38793250732124 fit data\n",
      "0.03701113257557154 predict data\n",
      "[14:16:45] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "1.1434778571128845e-05 getting data\n",
      "0.002969220280647278 transfering model\n",
      "29.159824670292437 fit data\n",
      "0.036344558000564575 predict data\n",
      "[14:17:15] Time limit exceeded after calculating fold 3\n",
      "[14:17:15] Time history [54.35030388832092, 25.02180814743042, 25.429642915725708, 29.201111555099487]. Time left -5.9921957687898555\n",
      "[14:17:15] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[14:17:15] Time left 428.55 secs\n",
      "\n",
      "[14:17:36] Start fitting Selector_CatBoost_gpu ...\n",
      "[14:17:36] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mSelector_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:17:56] Time history [20.42421317100525]. Time left 34.97019052505492\n",
      "[14:17:56] Selector_CatBoost_gpu fitting and predicting completed\n",
      "[14:18:46] Start fitting Lvl_0_Pipe_1_Mod_0_CatBoost_gpu ...\n",
      "[14:19:00] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:19:01] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:19:39] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:19:40] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:20:00] Time limit exceeded after calculating fold(s) [2 3]\n",
      "[14:20:00] Time history [20.42421317100525, 37.20497131347656, 36.73677611351013]. Time left -21.57884421348573\n",
      "[14:20:42] Lvl_0_Pipe_1_Mod_0_CatBoost_gpu fitting and predicting completed\n",
      "[14:20:42] Start fitting Lvl_0_Pipe_1_Mod_1_XGB ...\n",
      "[14:20:55] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Bootstrap : Using eth0:10.233.125.199<0>\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO NET/IB : No device found.\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO NET/Socket : Using [0]eth0:10.233.125.199<0>\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Using network Socket\n",
      "NCCL version 2.11.4+cuda11.0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Setting affinity for GPU 3 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:57358:60113 [1] NCCL INFO comm 0x7fbd5c49a2a0 rank 0 nranks 1 cudaDev 1 busId e5000 - Init COMPLETE\n",
      "[14:20:57] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Setting affinity for GPU 2 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:57358:60112 [0] NCCL INFO comm 0x7fbebfa04330 rank 0 nranks 1 cudaDev 0 busId be000 - Init COMPLETE\n",
      "13.222482020966709 xgb single fold time\n",
      "15.922786223702133 xgb single fold time\n",
      "[14:21:28] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[14:21:28] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Setting affinity for GPU 2 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Setting affinity for GPU 3 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:57358:60163 [0] NCCL INFO comm 0x7fb99d15f350 rank 0 nranks 1 cudaDev 0 busId be000 - Init COMPLETE\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:57358:60164 [1] NCCL INFO comm 0x7fbbfb717950 rank 0 nranks 1 cudaDev 1 busId e5000 - Init COMPLETE\n",
      "12.349738989025354 xgb single fold time\n",
      "13.507698575034738 xgb single fold time\n",
      "[14:21:50] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Setting affinity for GPU 2 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:57358:60216 [0] NCCL INFO comm 0x7fbbf8abd800 rank 0 nranks 1 cudaDev 0 busId be000 - Init COMPLETE\n",
      "14.909833048470318 xgb single fold time\n",
      "[14:22:05] Time history [29.456775188446045, 30.09367346763611, 23.573009252548218]. Time left 66.05814814567563\n",
      "[14:22:47] Lvl_0_Pipe_1_Mod_1_XGB fitting and predicting completed\n",
      "[14:22:47] Time left 96.60 secs\n",
      "\n",
      "[14:22:47] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[14:22:47] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:22:47] Blending: Optimization starts with equal weights and score 0.7747510075569153\n",
      "[14:22:49] Blending, iter 0: score = 0.77704918384552, weights = [0.         0.53037614 0.46962383]\n",
      "[14:22:52] Blending, iter 1: score = 0.77704918384552, weights = [0.         0.53037614 0.46962383]\n",
      "[14:22:52] No score update. Terminated\n",
      "[14:22:52] \u001b[1mAutoml preset training completed in 628.97 seconds\u001b[0m\n",
      "\n",
      "[14:22:52] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.53038 * (4 averaged models Lvl_0_Pipe_1_Mod_0_CatBoost_gpu) +\n",
      "\t 0.46962 * (5 averaged models Lvl_0_Pipe_1_Mod_1_XGB) \n",
      "\n",
      "Warning: less than 75% gpu memory available for training. Free: 23660.25 Total: 32510.5\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe.Warning: less than 75% gpu memory available for training. Free: 22706.25 Total: 32510.5\n",
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe.Warning: less than 75% gpu memory available for training. Free: 22420.25 Total: 32510.5\n",
      "Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 106, in <module>\n",
      "    test_pred = automl.predict(test.reset_index(drop=True)).data\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 604, in predict\n",
      "    pred = super(TabularAutoML_gpu.__bases__[0], self).predict(data, features_names, return_all_predictions)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 298, in predict\n",
      "    level_predictions.append(ml_pipe.predict(dataset))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/ml/base.py\", line 161, in predict\n",
      "    pred = model.predict(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/base_gpu.py\", line 240, in predict\n",
      "    pred = self.predict_single_fold(model, dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/boost_cb_gpu.py\", line 345, in predict_single_fold\n",
      "    pred = self._predict(model, cb_test, params)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/boost_cb_gpu.py\", line 388, in _predict\n",
      "    pred = model.predict(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/catboost/core.py\", line 2324, in predict\n",
      "    return self._predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, 'predict', task_type)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/catboost/core.py\", line 2272, in _predict\n",
      "    predictions = self._base_predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/catboost/core.py\", line 1568, in _base_predict\n",
      "    return self._object._base_predict(pool, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n",
      "  File \"_catboost.pyx\", line 4538, in _catboost._CatBoost._base_predict\n",
      "  File \"_catboost.pyx\", line 4545, in _catboost._CatBoost._base_predict\n",
      "_catboost.CatBoostError: catboost/libs/data/model_dataset_compatibility.cpp:81: At position 0 should be feature with name le_gpu__V15 (found ord_gpu__V37).\n",
      "\n",
      "Processing higgs_0\n",
      "Processing guillermo_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARD TIMEOUT!\n",
      "Processing bank-marketing_0\n",
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset bank-marketing, fold 0\n",
      "   V1            V2       V3         V4  V5  ...  V13 V14 V15      V16  Class\n",
      "0  58    management  married   tertiary  no  ...    1  -1   0  unknown      0\n",
      "1  44    technician   single  secondary  no  ...    1  -1   0  unknown      0\n",
      "2  33  entrepreneur  married  secondary  no  ...    1  -1   0  unknown      0\n",
      "3  47   blue-collar  married    unknown  no  ...    1  -1   0  unknown      0\n",
      "4  33       unknown   single    unknown  no  ...    1  -1   0  unknown      0\n",
      "\n",
      "[5 rows x 17 columns]\n",
      "setting gpuids with all\n",
      "[14:24:20] Stdout logging level is DEBUG.\n",
      "[14:24:20] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:24:20] Task: binary\n",
      "\n",
      "[14:24:20] Start automl preset with listed constraints:\n",
      "[14:24:20] - time: 720.00 seconds\n",
      "[14:24:20] - CPU: 4 cores\n",
      "[14:24:20] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 4\n",
      "[14:24:28] Train data shape: (40689, 17)\n",
      "daskcudf reader: 0.3569756867364049\n",
      "hybrid reader: 9.223625773563981\n",
      "[14:24:29] Layer \u001b[1m1\u001b[0m train process start. Time left 710.77 secs\n",
      "[14:24:46] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[14:24:47] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "pandas reader: 0.27228017896413803\n",
      "pandas reader: 0.33723102416843176\n",
      "pandas reader: 0.25620512943714857\n",
      "pandas reader: 0.2760143242776394\n",
      "cudf reader: 3.898779640905559\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "cudf reader: 3.929937389679253\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "7.282942533493042e-06 getting data\n",
      "3.7219035988673568 transfering model\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "37.58009572792798 fit data\n",
      "0.01174806710332632 predict data\n",
      "[14:25:29] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "1.161918044090271e-05 getting data\n",
      "0.0018779747188091278 transfering model\n",
      "21.997176064178348 fit data\n",
      "0.013071550987660885 predict data\n",
      "[14:25:51] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "8.382834494113922e-06 getting data\n",
      "0.0013617556542158127 transfering model\n",
      "5.250171377323568 fit data\n",
      "0.013998148031532764 predict data\n",
      "[14:25:57] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "7.916241884231567e-06 getting data\n",
      "0.0013238731771707535 transfering model\n",
      "7.0591734033077955 fit data\n",
      "0.026773275807499886 predict data\n",
      "[14:26:05] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "1.0468065738677979e-05 getting data\n",
      "0.0013733450323343277 transfering model\n",
      "5.191279442049563 fit data\n",
      "0.010829388163983822 predict data\n",
      "[14:26:10] Time history [41.321887493133545, 22.013009548187256, 5.266368627548218, 7.088115930557251, 5.204242944717407]. Time left 113.79590779001057\n",
      "[14:26:10] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[14:26:10] Time left 609.71 secs\n",
      "\n",
      "[14:26:13] Start fitting Selector_CatBoost_gpu ...\n",
      "[14:26:13] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mSelector_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:26:23] Time history [9.523193120956421]. Time left 79.54052992661792\n",
      "[14:26:23] Selector_CatBoost_gpu fitting and predicting completed\n",
      "[14:26:30] Start fitting Lvl_0_Pipe_1_Mod_0_CatBoost_gpu ...\n",
      "[14:26:31] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:26:31] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:26:43] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:26:43] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:26:52] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:26:58] Time history [9.523193120956421, 11.690207719802856, 9.424031019210815, 6.144437074661255]. Time left 76.1716800689697\n",
      "[14:27:01] Lvl_0_Pipe_1_Mod_0_CatBoost_gpu fitting and predicting completed\n",
      "[14:27:01] Start fitting Lvl_0_Pipe_1_Mod_1_XGB ...\n",
      "[14:27:02] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[14:27:02] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Bootstrap : Using eth0:10.233.125.199<0>\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO NET/IB : No device found.\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO NET/Socket : Using [0]eth0:10.233.125.199<0>\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Using network Socket\n",
      "NCCL version 2.11.4+cuda11.0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Setting affinity for GPU 1 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:61002:62376 [1] NCCL INFO comm 0x7f780d1de1b0 rank 0 nranks 1 cudaDev 1 busId bc000 - Init COMPLETE\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:61002:62375 [0] NCCL INFO comm 0x7f7d7f795370 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "4.0669128047302365 xgb single fold time\n",
      "4.122238153591752 xgb single fold time\n",
      "[14:27:07] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[14:27:07] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Setting affinity for GPU 1 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:61002:62398 [1] NCCL INFO comm 0x7f7d7f898810 rank 0 nranks 1 cudaDev 1 busId bc000 - Init COMPLETE\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:61002:62397 [0] NCCL INFO comm 0x7f780dd80090 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "3.6802488155663013 xgb single fold time\n",
      "3.776364547200501 xgb single fold time\n",
      "[14:27:11] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:61002:62418 [0] NCCL INFO comm 0x7f780d2e5280 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "4.383333508856595 xgb single fold time\n",
      "[14:27:16] Time history [5.289596080780029, 4.871788263320923, 4.971269130706787]. Time left 471.9321444034575\n",
      "[14:27:19] Lvl_0_Pipe_1_Mod_1_XGB fitting and predicting completed\n",
      "[14:27:19] Time left 540.78 secs\n",
      "\n",
      "[14:27:19] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:27:19] Blending: Optimization starts with equal weights and score 0.9243718385696411\n",
      "[14:27:19] Blending, iter 0: score = 0.9269363284111023, weights = [0.         0.65551925 0.34448072]\n",
      "[14:27:19] Blending, iter 1: score = 0.9269363284111023, weights = [0.         0.6555193  0.34448072]\n",
      "[14:27:20] Blending, iter 2: score = 0.9269363284111023, weights = [0.         0.6555193  0.34448072]\n",
      "[14:27:20] No score update. Terminated\n",
      "[14:27:20] \u001b[1mAutoml preset training completed in 180.48 seconds\u001b[0m\n",
      "\n",
      "[14:27:20] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.65552 * (5 averaged models Lvl_0_Pipe_1_Mod_0_CatBoost_gpu) +\n",
      "\t 0.34448 * (5 averaged models Lvl_0_Pipe_1_Mod_1_XGB) \n",
      "\n",
      "Warning: less than 75% gpu memory available for training. Free: 20921.25 Total: 32510.5\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "Warning: less than 75% gpu memory available for training. Free: 20455.25 Total: 32510.5\n",
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe.Custom logger is already specified. Specify more than one logger at same time is not thread safe.Warning: less than 75% gpu memory available for training. Free: 20731.25 Total: 32510.5\n",
      "Warning: less than 75% gpu memory available for training. Free: 20725.25 Total: 32510.5\n",
      "Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 106, in <module>\n",
      "    test_pred = automl.predict(test.reset_index(drop=True)).data\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 604, in predict\n",
      "    pred = super(TabularAutoML_gpu.__bases__[0], self).predict(data, features_names, return_all_predictions)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 298, in predict\n",
      "    level_predictions.append(ml_pipe.predict(dataset))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/ml/base.py\", line 161, in predict\n",
      "    pred = model.predict(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/base_gpu.py\", line 240, in predict\n",
      "    pred = self.predict_single_fold(model, dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/boost_cb_gpu.py\", line 345, in predict_single_fold\n",
      "    pred = self._predict(model, cb_test, params)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/boost_cb_gpu.py\", line 388, in _predict\n",
      "    pred = model.predict(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/catboost/core.py\", line 2324, in predict\n",
      "    return self._predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, 'predict', task_type)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/catboost/core.py\", line 2272, in _predict\n",
      "    predictions = self._base_predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/catboost/core.py\", line 1568, in _base_predict\n",
      "    return self._object._base_predict(pool, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n",
      "  File \"_catboost.pyx\", line 4538, in _catboost._CatBoost._base_predict\n",
      "  File \"_catboost.pyx\", line 4545, in _catboost._CatBoost._base_predict\n",
      "_catboost.CatBoostError: catboost/libs/data/model_dataset_compatibility.cpp:81: At position 3 should be feature with name le_gpu__V11 (found ord_gpu__V8).\n",
      "\n",
      "Processing numerai28.6_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset numerai28.6, fold 0\n",
      "   attribute_0  attribute_1  ...  attribute_20  attribute_21\n",
      "0     0.497449     0.722213  ...      0.639446             0\n",
      "1     0.314430     0.442740  ...      0.216417             1\n",
      "2     0.542753     0.749818  ...      0.985990             1\n",
      "3     0.789343     0.624176  ...      0.664026             0\n",
      "4     0.046066     0.815840  ...      0.217831             1\n",
      "\n",
      "[5 rows x 22 columns]\n",
      "setting gpuids with all\n",
      "[14:27:38] Stdout logging level is DEBUG.\n",
      "[14:27:38] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:27:38] Task: binary\n",
      "\n",
      "[14:27:38] Start automl preset with listed constraints:\n",
      "[14:27:38] - time: 720.00 seconds\n",
      "[14:27:38] - CPU: 4 cores\n",
      "[14:27:38] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 4\n",
      "[14:27:47] Train data shape: (86688, 22)\n",
      "daskcudf reader: 0.38743586651980877\n",
      "hybrid reader: 9.28884949348867\n",
      "[14:27:47] Layer \u001b[1m1\u001b[0m train process start. Time left 710.71 secs\n",
      "[14:27:52] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[14:27:53] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "pandas reader: 0.4375308593735099\n",
      "pandas reader: 0.43691700603812933\n",
      "pandas reader: 0.44112513586878777\n",
      "pandas reader: 0.7007755357772112\n",
      "cudf reader: 4.599360133521259\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "cudf reader: 4.594877132214606\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "8.422881364822388e-06 getting data\n",
      "3.4924403494223952 transfering model\n",
      "23.88790752645582 fit data\n",
      "0.011429445818066597 predict data\n",
      "[14:28:21] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "8.441507816314697e-06 getting data\n",
      "0.0006792088970541954 transfering model\n",
      "1.0673323990777135 fit data\n",
      "0.014378965832293034 predict data\n",
      "[14:28:23] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "9.561888873577118e-06 getting data\n",
      "0.0005874130874872208 transfering model\n",
      "1.2745924890041351 fit data\n",
      "0.014399672858417034 predict data\n",
      "[14:28:25] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "1.8602237105369568e-05 getting data\n",
      "0.000705263577401638 transfering model\n",
      "1.0605668583884835 fit data\n",
      "0.011563656851649284 predict data\n",
      "[14:28:27] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "2.598017454147339e-05 getting data\n",
      "0.0005923835560679436 transfering model\n",
      "2.001670683734119 fit data\n",
      "0.014880849979817867 predict data\n",
      "[14:28:29] Time history [27.404064893722534, 1.0832955837249756, 1.2904632091522217, 1.0737297534942627, 2.018183469772339]. Time left 164.21211276271123\n",
      "[14:28:29] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[14:28:29] Time left 668.27 secs\n",
      "\n",
      "[14:28:31] Start fitting Selector_CatBoost_gpu ...\n",
      "[14:28:31] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mSelector_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:28:34] Time history [2.6124558448791504]. Time left 96.43389666080472\n",
      "[14:28:34] Selector_CatBoost_gpu fitting and predicting completed\n",
      "[14:28:35] Start fitting Lvl_0_Pipe_1_Mod_0_CatBoost_gpu ...\n",
      "[14:28:37] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:28:37] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:28:41] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:28:41] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:28:44] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:28:47] Time history [2.6124558448791504, 4.154971361160278, 4.035823106765747, 3.173701047897339]. Time left 106.71275992393491\n",
      "[14:28:51] Lvl_0_Pipe_1_Mod_0_CatBoost_gpu fitting and predicting completed\n",
      "[14:28:51] Start fitting Lvl_0_Pipe_1_Mod_1_XGB ...\n",
      "[14:28:52] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[14:28:52] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Bootstrap : Using eth0:10.233.125.199<0>\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO NET/IB : No device found.\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO NET/Socket : Using [0]eth0:10.233.125.199<0>\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Using network Socket\n",
      "NCCL version 2.11.4+cuda11.0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:62492:63679 [0] NCCL INFO comm 0x7f579b770000 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Setting affinity for GPU 1 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:62492:63680 [1] NCCL INFO comm 0x7f57b9e93af0 rank 0 nranks 1 cudaDev 1 busId bc000 - Init COMPLETE\n",
      "2.2793351458385587 xgb single fold time\n",
      "2.3740434236824512 xgb single fold time\n",
      "[14:28:56] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[14:28:57] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Setting affinity for GPU 1 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:62492:63701 [1] NCCL INFO comm 0x7f57ba3a1c70 rank 0 nranks 1 cudaDev 1 busId bc000 - Init COMPLETE\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:62492:63700 [0] NCCL INFO comm 0x7f579d66d580 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "2.4246885776519775 xgb single fold time\n",
      "2.5748044438660145 xgb single fold time\n",
      "[14:29:00] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:62492:63721 [0] NCCL INFO comm 0x7f5e4019e970 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "1.9941969690844417 xgb single fold time\n",
      "[14:29:02] Time history [4.033737897872925, 4.128902912139893, 2.8611717224121094]. Time left 563.835369825363\n",
      "[14:29:06] Lvl_0_Pipe_1_Mod_1_XGB fitting and predicting completed\n",
      "[14:29:06] Time left 631.41 secs\n",
      "\n",
      "[14:29:06] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:29:07] Blending: Optimization starts with equal weights and score 0.5299760103225708\n",
      "[14:29:07] Blending, iter 0: score = 0.5321023464202881, weights = [0.48130932 0.5186907  0.        ]\n",
      "[14:29:08] Blending, iter 1: score = 0.5321058034896851, weights = [0.50556 0.49444 0.     ]\n",
      "[14:29:08] Blending, iter 2: score = 0.5321058034896851, weights = [0.50556 0.49444 0.     ]\n",
      "[14:29:08] No score update. Terminated\n",
      "[14:29:08] \u001b[1mAutoml preset training completed in 90.26 seconds\u001b[0m\n",
      "\n",
      "[14:29:08] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.50556 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.49444 * (5 averaged models Lvl_0_Pipe_1_Mod_0_CatBoost_gpu) \n",
      "\n",
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe.Custom logger is already specified. Specify more than one logger at same time is not thread safe.Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 106, in <module>\n",
      "    test_pred = automl.predict(test.reset_index(drop=True)).data\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 604, in predict\n",
      "    pred = super(TabularAutoML_gpu.__bases__[0], self).predict(data, features_names, return_all_predictions)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 298, in predict\n",
      "    level_predictions.append(ml_pipe.predict(dataset))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/ml/base.py\", line 161, in predict\n",
      "    pred = model.predict(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/base_gpu.py\", line 240, in predict\n",
      "    pred = self.predict_single_fold(model, dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/linear_gpu.py\", line 237, in predict_single_fold\n",
      "    pred = model.predict(dataset_data, dev_id)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/torch_based/linear_model_distributed.py\", line 415, in predict\n",
      "    pred = super().predict(data)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/torch_based/linear_model_distributed.py\", line 337, in predict\n",
      "    data_num, data_cat, _, _ = self._prepare_data(data)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/torch_based/linear_model_distributed.py\", line 198, in _prepare_data\n",
      "    return self._prepare_data_dense(data, y, weights, rank)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/torch_based/linear_model_distributed.py\", line 117, in _prepare_data_dense\n",
      "    data_ = torch.as_tensor(data.values.astype(cp.float32), device=device_id)\n",
      "AttributeError: 'cupy._core.core.ndarray' object has no attribute 'values'\n",
      "\n",
      "Processing volkert_0\n",
      "Processing adult_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset adult, fold 0\n",
      "   age   workclass  fnlwgt  ... hours-per-week  native-country class\n",
      "0   25     Private  226802  ...             40   United-States     0\n",
      "1   38     Private   89814  ...             50   United-States     0\n",
      "2   28   Local-gov  336951  ...             40   United-States     1\n",
      "3   44     Private  160323  ...             40   United-States     1\n",
      "4   18           ?  103497  ...             30   United-States     0\n",
      "\n",
      "[5 rows x 15 columns]\n",
      "setting gpuids with all\n",
      "[14:29:25] Stdout logging level is DEBUG.\n",
      "[14:29:25] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:29:25] Task: binary\n",
      "\n",
      "[14:29:25] Start automl preset with listed constraints:\n",
      "[14:29:25] - time: 720.00 seconds\n",
      "[14:29:25] - CPU: 4 cores\n",
      "[14:29:25] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 4\n",
      "[14:29:33] Train data shape: (43957, 15)\n",
      "daskcudf reader: 0.4879192840307951\n",
      "hybrid reader: 8.486936865374446\n",
      "[14:29:34] Layer \u001b[1m1\u001b[0m train process start. Time left 711.51 secs\n",
      "[14:29:44] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[14:29:45] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "pandas reader: 0.17666510213166475\n",
      "pandas reader: 0.19250913616269827\n",
      "pandas reader: 0.20013028290122747\n",
      "pandas reader: 0.3780503449961543\n",
      "cudf reader: 3.6656614309176803\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "cudf reader: 3.6909509282559156\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "2.3568980395793915e-05 getting data\n",
      "3.5256505450233817 transfering model\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "51.75468880776316 fit data\n",
      "0.008604028262197971 predict data\n",
      "[14:30:41] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "7.319264113903046e-06 getting data\n",
      "0.0016745291650295258 transfering model\n",
      "6.367882734164596 fit data\n",
      "0.010677531361579895 predict data\n",
      "[14:30:48] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "7.609836757183075e-06 getting data\n",
      "0.0012442152947187424 transfering model\n",
      "7.785510580986738 fit data\n",
      "0.010410133749246597 predict data\n",
      "[14:30:56] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "8.082948625087738e-06 getting data\n",
      "0.001191827468574047 transfering model\n",
      "6.605987091548741 fit data\n",
      "0.010045666247606277 predict data\n",
      "[14:31:03] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "8.168630301952362e-06 getting data\n",
      "0.0011984286829829216 transfering model\n",
      "9.134985430166125 fit data\n",
      "0.010027593933045864 predict data\n",
      "[14:31:12] Time history [55.29726719856262, 6.380961894989014, 7.797875881195068, 6.617956638336182, 9.146965026855469]. Time left 112.13790230317545\n",
      "[14:31:12] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[14:31:12] Time left 613.12 secs\n",
      "\n",
      "[14:31:15] Start fitting Selector_CatBoost_gpu ...\n",
      "[14:31:15] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mSelector_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:31:23] Time history [8.046377420425415]. Time left 81.60775661468504\n",
      "[14:31:23] Selector_CatBoost_gpu fitting and predicting completed\n",
      "[14:31:31] Start fitting Lvl_0_Pipe_1_Mod_0_CatBoost_gpu ...\n",
      "[14:31:32] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:31:32] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:31:38] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:31:38] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:31:45] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:31:50] Time history [8.046377420425415, 5.933988571166992, 7.034788608551025, 5.5028157234191895]. Time left 85.93084907531735\n",
      "[14:31:52] Lvl_0_Pipe_1_Mod_0_CatBoost_gpu fitting and predicting completed\n",
      "[14:31:52] Start fitting Lvl_0_Pipe_1_Mod_1_XGB ...\n",
      "[14:31:53] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[14:31:54] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Bootstrap : Using eth0:10.233.125.199<0>\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO NET/IB : No device found.\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO NET/Socket : Using [0]eth0:10.233.125.199<0>\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Using network Socket\n",
      "NCCL version 2.11.4+cuda11.0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Setting affinity for GPU 1 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:63775:65209 [1] NCCL INFO comm 0x7fcdca8e3890 rank 0 nranks 1 cudaDev 1 busId bc000 - Init COMPLETE\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:63775:65208 [0] NCCL INFO comm 0x7fcda8def290 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "2.963159159757197 xgb single fold time\n",
      "3.3476580679416656 xgb single fold time\n",
      "[14:31:58] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[14:31:58] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Setting affinity for GPU 1 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:63775:65229 [0] NCCL INFO comm 0x7fcda8dce210 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:63775:65230 [1] NCCL INFO comm 0x7fcddc0e8dc0 rank 0 nranks 1 cudaDev 1 busId bc000 - Init COMPLETE\n",
      "3.133468714542687 xgb single fold time\n",
      "3.176886063069105 xgb single fold time\n",
      "[14:32:02] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:63775:65250 [0] NCCL INFO comm 0x7fcddc2fd5a0 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "3.1256343172863126 xgb single fold time\n",
      "[14:32:05] Time history [4.426880359649658, 4.281736612319946, 3.7060911655426025]. Time left 488.5063500404357\n",
      "[14:32:08] Lvl_0_Pipe_1_Mod_1_XGB fitting and predicting completed\n",
      "[14:32:08] Time left 557.60 secs\n",
      "\n",
      "[14:32:08] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:32:08] Blending: Optimization starts with equal weights and score 0.9254533052444458\n",
      "[14:32:08] Blending, iter 0: score = 0.9259028434753418, weights = [0.10033503 0.58057153 0.31909344]\n",
      "[14:32:09] Blending, iter 1: score = 0.9259032607078552, weights = [0.0968515  0.5828195  0.32032895]\n",
      "[14:32:09] Blending, iter 2: score = 0.9259032607078552, weights = [0.0968515  0.5828195  0.32032895]\n",
      "[14:32:09] No score update. Terminated\n",
      "[14:32:09] \u001b[1mAutoml preset training completed in 163.48 seconds\u001b[0m\n",
      "\n",
      "[14:32:09] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.09685 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.58282 * (5 averaged models Lvl_0_Pipe_1_Mod_0_CatBoost_gpu) +\n",
      "\t 0.32033 * (5 averaged models Lvl_0_Pipe_1_Mod_1_XGB) \n",
      "\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe.Custom logger is already specified. Specify more than one logger at same time is not thread safe.Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 106, in <module>\n",
      "    test_pred = automl.predict(test.reset_index(drop=True)).data\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 604, in predict\n",
      "    pred = super(TabularAutoML_gpu.__bases__[0], self).predict(data, features_names, return_all_predictions)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 298, in predict\n",
      "    level_predictions.append(ml_pipe.predict(dataset))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/ml/base.py\", line 155, in predict\n",
      "    dataset = self.features_pipeline.transform(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/features/base.py\", line 129, in transform\n",
      "    return self._pipeline.transform(test)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 331, in transform\n",
      "    res = self._transform_singleproc(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 302, in _transform_singleproc\n",
      "    ds = trf.transform(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 140, in transform\n",
      "    dataset = trf.transform(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 331, in transform\n",
      "    res = self._transform_singleproc(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 302, in _transform_singleproc\n",
      "    ds = trf.transform(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 140, in transform\n",
      "    dataset = trf.transform(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/numeric_gpu.py\", line 272, in transform\n",
      "    return self._transform_daskcudf(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/numeric_gpu.py\", line 250, in _transform_daskcudf\n",
      "    data = dataset.data.map_partitions(self._inf_to_nan,\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/base.py\", line 261, in persist\n",
      "    (result,) = persist(self, traverse=False, **kwargs)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/base.py\", line 778, in persist\n",
      "    results = schedule(dsk, keys, **kwargs)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/local.py\", line 563, in get_sync\n",
      "    return get_async(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/local.py\", line 506, in get_async\n",
      "    for key, res_info, failed in queue_get(queue).result():\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/concurrent/futures/_base.py\", line 437, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/local.py\", line 548, in submit\n",
      "    fut.set_result(fn(*args, **kwargs))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/local.py\", line 237, in batch_execute_tasks\n",
      "    return [execute_task(*a) for a in it]\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/local.py\", line 237, in <listcomp>\n",
      "    return [execute_task(*a) for a in it]\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/local.py\", line 228, in execute_task\n",
      "    result = pack_exception(e, dumps)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/local.py\", line 223, in execute_task\n",
      "    result = _execute_task(task, data)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/core.py\", line 121, in _execute_task\n",
      "    return func(*(_execute_task(a, cache) for a in args))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/optimization.py\", line 969, in __call__\n",
      "    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/core.py\", line 151, in get\n",
      "    result = _execute_task(task, cache)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/core.py\", line 121, in _execute_task\n",
      "    return func(*(_execute_task(a, cache) for a in args))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/utils.py\", line 35, in apply\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/dataframe/core.py\", line 5931, in apply_and_enforce\n",
      "    df = func(*args, **kwargs)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/numeric_gpu.py\", line 229, in _inf_to_nan\n",
      "    output = cp.where(cp.isinf(data.values), cp.nan, data.values)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/dataframe.py\", line 1006, in values\n",
      "    return cupy.asarray(self.as_gpu_matrix())\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/dataframe.py\", line 3729, in as_gpu_matrix\n",
      "    raise ValueError(\n",
      "ValueError: column 'logodds_gpu__le_gpu__age' has null values. hint: use .fillna() to replace null values\n",
      "\n",
      "Processing MiniBooNE_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset MiniBooNE, fold 0\n",
      "   signal  ParticleID_0  ...  ParticleID_48  ParticleID_49\n",
      "0       1       2.59413  ...       0.071769       0.245996\n",
      "1       1       3.86388  ...       0.333613       0.230621\n",
      "2       1       3.38584  ...       0.255512       0.180901\n",
      "3       1       4.28524  ...       0.473081       0.258990\n",
      "4       1       5.93662  ...       1.924990       0.253893\n",
      "\n",
      "[5 rows x 51 columns]\n",
      "setting gpuids with all\n",
      "[14:32:27] Stdout logging level is DEBUG.\n",
      "[14:32:27] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:32:27] Task: binary\n",
      "\n",
      "[14:32:27] Start automl preset with listed constraints:\n",
      "[14:32:27] - time: 720.00 seconds\n",
      "[14:32:27] - CPU: 4 cores\n",
      "[14:32:27] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 4\n",
      "[14:32:37] Train data shape: (117057, 51)\n",
      "daskcudf reader: 1.1639997903257608\n",
      "hybrid reader: 10.791778171434999\n",
      "[14:32:38] Layer \u001b[1m1\u001b[0m train process start. Time left 709.20 secs\n",
      "[14:32:52] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[14:32:54] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "pandas reader: 1.1134817944839597\n",
      "pandas reader: 1.2537849927321076\n",
      "pandas reader: 1.2609965624287724\n",
      "pandas reader: 3.256332790479064\n",
      "cudf reader: 5.457060483284295\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "cudf reader: 5.411667128093541\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "8.155591785907745e-06 getting data\n",
      "3.6624620743095875 transfering model\n",
      "Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 99, in <module>\n",
      "    oof_predictions = automl.fit_predict(train.reset_index(drop=True),\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 558, in fit_predict\n",
      "    oof_pred = super(TabularAutoML_gpu.__bases__[0], self).fit_predict(train, roles=roles, cv_iter=cv_iter, valid_data=valid_data, verbose=verbose)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/base.py\", line 204, in fit_predict\n",
      "    result = super().fit_predict(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 218, in fit_predict\n",
      "    pipe_pred = ml_pipe.fit_predict(train_valid)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/ml/base.py\", line 129, in fit_predict\n",
      "    ml_algo, preds = tune_and_fit_predict(ml_algo, param_tuner, train_valid, force_calc)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/utils.py\", line 66, in tune_and_fit_predict\n",
      "    preds = ml_algo.fit_predict(train_valid)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/base_gpu.py\", line 179, in fit_predict\n",
      "    model, pred = self.fit_predict_single_fold(train, valid)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/linear_gpu.py\", line 205, in fit_predict_single_fold\n",
      "    model.fit(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/torch_based/linear_model_distributed.py\", line 315, in fit\n",
      "    res = p(delayed(score_iteration)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/multiprocessing/pool.py\", line 771, in get\n",
      "    raise self._value\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/torch_based/linear_model_distributed.py\", line 291, in score_iteration\n",
      "    score = self.metric(y_val, val_pred, weights_val)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/tasks/losses/base.py\", line 36, in __call__\n",
      "    val = self.metric_func(y_true, y_pred, sample_weight=sample_weight)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/tasks/common_metric_gpu.py\", line 61, in roc_auc_score_gpu\n",
      "    res = roc_auc_score(y_true, y_pred)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cuml/internals/api_decorators.py\", line 360, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cuml/metrics/_ranking.py\", line 161, in roc_auc_score\n",
      "    return _binary_roc_auc_score(y_true, y_score)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cuml/metrics/_ranking.py\", line 196, in _binary_roc_auc_score\n",
      "    raise ValueError(\"roc_auc_score cannot be used when \"\n",
      "ValueError: roc_auc_score cannot be used when only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "Processing dilbert_0\n",
      "HARD TIMEOUT!\n",
      "Processing riccardo_0\n",
      "HARD TIMEOUT!\n",
      "Processing shuttle_0\n",
      "HARD TIMEOUT!\n",
      "Processing KDDCup09_appetency_0\n",
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset KDDCup09_appetency, fold 0\n",
      "   Var1  Var2  Var3  Var4  ...         Var228  Var229  Var230  APPETENCY\n",
      "0   NaN   NaN   NaN   NaN  ...  F2FyR07IdsN7I     NaN     NaN          0\n",
      "1   NaN   NaN   NaN   NaN  ...  F2FyR07IdsN7I     NaN     NaN          0\n",
      "2   NaN   NaN   NaN   NaN  ...  ib5G6X1eUxUn6    am7c     NaN          0\n",
      "3   NaN   NaN   NaN   NaN  ...  F2FyR07IdsN7I     NaN     NaN          0\n",
      "4   NaN   NaN   NaN   NaN  ...  F2FyR07IdsN7I    mj86     NaN          0\n",
      "\n",
      "[5 rows x 231 columns]\n",
      "setting gpuids with all\n",
      "[14:47:16] Stdout logging level is DEBUG.\n",
      "[14:47:16] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:47:16] Task: binary\n",
      "\n",
      "[14:47:16] Start automl preset with listed constraints:\n",
      "[14:47:16] - time: 720.00 seconds\n",
      "[14:47:16] - CPU: 4 cores\n",
      "[14:47:16] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 4\n",
      "[14:47:38] Train data shape: (45000, 231)\n",
      "daskcudf reader: 2.574116798117757\n",
      "hybrid reader: 24.71725663356483\n",
      "[14:47:40] Layer \u001b[1m1\u001b[0m train process start. Time left 695.28 secs\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 99, in <module>\n",
      "    oof_predictions = automl.fit_predict(train.reset_index(drop=True),\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 558, in fit_predict\n",
      "    oof_pred = super(TabularAutoML_gpu.__bases__[0], self).fit_predict(train, roles=roles, cv_iter=cv_iter, valid_data=valid_data, verbose=verbose)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/base.py\", line 204, in fit_predict\n",
      "    result = super().fit_predict(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 218, in fit_predict\n",
      "    pipe_pred = ml_pipe.fit_predict(train_valid)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/ml/base.py\", line 123, in fit_predict\n",
      "    train_valid = train_valid.apply_feature_pipeline(self.features_pipeline)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/validation/base.py\", line 81, in apply_feature_pipeline\n",
      "    train_valid.train = features_pipeline.fit_transform(train_valid.train)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/features/base.py\", line 117, in fit_transform\n",
      "    return self._pipeline.fit_transform(train)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 281, in fit_transform\n",
      "    res = self._fit_transform_singleproc(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 243, in _fit_transform_singleproc\n",
      "    ds = trf.fit_transform(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 157, in fit_transform\n",
      "    dataset = trf.fit_transform(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 281, in fit_transform\n",
      "    res = self._fit_transform_singleproc(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 243, in _fit_transform_singleproc\n",
      "    ds = trf.fit_transform(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 157, in fit_transform\n",
      "    dataset = trf.fit_transform(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/base.py\", line 98, in fit_transform\n",
      "    self.fit(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/numeric_gpu.py\", line 173, in fit\n",
      "    self._fit_daskcudf(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/transformers/numeric_gpu.py\", line 155, in _fit_daskcudf\n",
      "    self.meds = dataset.data.dropna().quantile().compute().astype(np.float32).fillna(0.0)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/base.py\", line 288, in compute\n",
      "    (result,) = compute(self, traverse=False, **kwargs)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/base.py\", line 570, in compute\n",
      "    results = schedule(dsk, keys, **kwargs)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/threaded.py\", line 79, in get\n",
      "    results = get_async(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/local.py\", line 517, in get_async\n",
      "    raise_exception(exc, tb)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/local.py\", line 325, in reraise\n",
      "    raise exc\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/local.py\", line 223, in execute_task\n",
      "    result = _execute_task(task, data)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/core.py\", line 121, in _execute_task\n",
      "    return func(*(_execute_task(a, cache) for a in args))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/core.py\", line 121, in <genexpr>\n",
      "    return func(*(_execute_task(a, cache) for a in args))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/core.py\", line 121, in _execute_task\n",
      "    return func(*(_execute_task(a, cache) for a in args))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/dask/array/percentile.py\", line 210, in merge_percentiles\n",
      "    raise ValueError(\"No non-trivial arrays found\")\n",
      "ValueError: No non-trivial arrays found\n",
      "pandas reader: 0.37488067988306284\n",
      "pandas reader: 0.4017495382577181\n",
      "pandas reader: 0.30857596173882484\n",
      "pandas reader: 5.8283334989100695\n",
      "cudf reader: 17.600821390748024\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "cudf reader: 12.576125620864332\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "\n",
      "Processing Fashion-MNIST_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset shuttle, fold 0\n",
      "   A1  A2  A3  A4  A5  A6  A7  A8  A9  class\n",
      "0  50  21  77   0  28   0  27  48  22      1\n",
      "1  55   0  92   0   0  26  36  92  56      3\n",
      "2  53   0  82   0  52  -5  29  30   2      0\n",
      "3  37   0  76   0  28  18  40  48   8      0\n",
      "4  37   0  79   0  34 -26  43  46   2      0\n",
      "setting gpuids with all\n",
      "[14:45:34] Stdout logging level is DEBUG.\n",
      "[14:45:34] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:45:34] Task: multiclass\n",
      "\n",
      "[14:45:34] Start automl preset with listed constraints:\n",
      "[14:45:34] - time: 720.00 seconds\n",
      "[14:45:34] - CPU: 4 cores\n",
      "[14:45:34] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 4\n",
      "[14:45:48] Train data shape: (52200, 10)\n",
      "daskcudf reader: 0.35292075388133526\n",
      "hybrid reader: 15.016378089785576\n",
      "[14:45:49] Layer \u001b[1m1\u001b[0m train process start. Time left 704.97 secs\n",
      "[14:45:59] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[14:46:00] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "pandas reader: 0.6298059225082397\n",
      "pandas reader: 0.6199247045442462\n",
      "pandas reader: 0.6198602542281151\n",
      "pandas reader: 0.6151803201064467\n",
      "cudf reader: 9.92152651026845\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "cudf reader: 9.946641258895397\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "7.931143045425415e-06 getting data\n",
      "3.817632489837706 transfering model\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "123.71875369362533 fit data\n",
      "0.008750001899898052 predict data\n",
      "[14:48:08] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "8.893199265003204e-06 getting data\n",
      "0.0017215274274349213 transfering model\n",
      "40.9216618668288 fit data\n",
      "0.013993463478982449 predict data\n",
      "[14:48:49] Time limit exceeded after calculating fold 1\n",
      "[14:48:49] Time history [127.5631594657898, 40.93827199935913]. Time left -23.039494152004693\n",
      "[14:48:49] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[14:48:49] Time left 524.45 secs\n",
      "\n",
      "[14:48:51] Start fitting Selector_CatBoost_gpu ...\n",
      "[14:48:51] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mSelector_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:49:02] Time history [11.324015617370605]. Time left 38.49726457827912\n",
      "Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 99, in <module>\n",
      "    oof_predictions = automl.fit_predict(train.reset_index(drop=True),\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 558, in fit_predict\n",
      "    oof_pred = super(TabularAutoML_gpu.__bases__[0], self).fit_predict(train, roles=roles, cv_iter=cv_iter, valid_data=valid_data, verbose=verbose)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/base.py\", line 204, in fit_predict\n",
      "    result = super().fit_predict(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 218, in fit_predict\n",
      "    pipe_pred = ml_pipe.fit_predict(train_valid)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/ml/base.py\", line 121, in fit_predict\n",
      "    train_valid = train_valid.apply_selector(self.pre_selection)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/validation/base.py\", line 100, in apply_selector\n",
      "    selector.fit(self)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/selection/base.py\", line 185, in fit\n",
      "    self.ml_algo, preds = tune_and_fit_predict(self.ml_algo, self.tuner, train_valid)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/utils.py\", line 66, in tune_and_fit_predict\n",
      "    preds = ml_algo.fit_predict(train_valid)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/base_gpu.py\", line 172, in fit_predict\n",
      "    preds_arr[idx] += preds[n].reshape((preds[n].shape[0], -1))\n",
      "  File \"cupy/_core/core.pyx\", line 1224, in cupy._core.core.ndarray.__iadd__\n",
      "  File \"cupy/_core/_kernel.pyx\", line 1157, in cupy._core._kernel.ufunc.__call__\n",
      "  File \"cupy/_core/internal.pyx\", line 343, in cupy._core.internal._broadcast_core\n",
      "ValueError: operands could not be broadcast together with shapes (1, 10440, 6) (10440, 7) (1, 10440, 6)\n",
      "\n",
      "Processing connect-4_0\n",
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset connect-4, fold 0\n",
      "   a1  a2  a3  a4  a5  a6  b1  b2  b3  ...  f5  f6  g1  g2  g3  g4  g5  g6  class\n",
      "0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0      2\n",
      "1   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0      2\n",
      "2   0   0   0   0   0   0   1   0   0  ...   0   0   0   0   0   0   0   0      2\n",
      "3   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0      2\n",
      "4   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   0      2\n",
      "\n",
      "[5 rows x 43 columns]\n",
      "setting gpuids with all\n",
      "[14:49:18] Stdout logging level is DEBUG.\n",
      "[14:49:18] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:49:18] Task: multiclass\n",
      "\n",
      "[14:49:18] Start automl preset with listed constraints:\n",
      "[14:49:18] - time: 720.00 seconds\n",
      "[14:49:18] - CPU: 4 cores\n",
      "[14:49:18] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 4\n",
      "[14:49:28] Train data shape: (60801, 43)\n",
      "daskcudf reader: 0.851276814006269\n",
      "hybrid reader: 10.794899985194206\n",
      "[14:49:29] Layer \u001b[1m1\u001b[0m train process start. Time left 709.20 secs\n",
      "[14:49:39] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[14:49:39] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "pandas reader: 0.8705343883484602\n",
      "pandas reader: 0.8949406268075109\n",
      "pandas reader: 0.8685804205015302\n",
      "pandas reader: 1.651356947608292\n",
      "cudf reader: 5.428429220803082\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "cudf reader: 5.431496291421354\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "8.14441591501236e-06 getting data\n",
      "3.9025196032598615 transfering model\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "34.62846855260432 fit data\n",
      "0.020153090357780457 predict data\n",
      "[14:50:19] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "9.838491678237915e-06 getting data\n",
      "0.0008211927488446236 transfering model\n",
      "16.281309938058257 fit data\n",
      "0.030263285152614117 predict data\n",
      "[14:50:36] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "9.845942258834839e-06 getting data\n",
      "0.0006251605227589607 transfering model\n",
      "1.7873930437490344 fit data\n",
      "0.02629411593079567 predict data\n",
      "[14:50:39] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "7.727183401584625e-06 getting data\n",
      "0.0005945870652794838 transfering model\n",
      "1.8527172692120075 fit data\n",
      "0.026916620321571827 predict data\n",
      "[14:50:41] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "8.419156074523926e-06 getting data\n",
      "0.0006090821698307991 transfering model\n",
      "1.8002308076247573 fit data\n",
      "0.023387977853417397 predict data\n",
      "[14:50:43] Time history [38.57163763046265, 16.313255071640015, 1.8151881694793701, 1.8812601566314697, 1.8250467777252197]. Time left 83.87108489790475\n",
      "[14:50:43] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[14:50:43] Time left 635.29 secs\n",
      "\n",
      "[14:50:45] Start fitting Selector_CatBoost_gpu ...\n",
      "[14:50:45] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mSelector_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:50:58] Time history [12.731858253479004]. Time left 49.296307176615265\n",
      "[14:50:58] Selector_CatBoost_gpu fitting and predicting completed\n",
      "[14:51:01] Start fitting Lvl_0_Pipe_1_Mod_0_CatBoost_gpu ...\n",
      "[14:51:03] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:51:03] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:51:16] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:51:16] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:51:28] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:51:39] Time history [12.731858253479004, 13.030681848526001, 12.995792150497437, 11.515397071838379]. Time left 30.18889794776686\n",
      "[14:51:45] Lvl_0_Pipe_1_Mod_0_CatBoost_gpu fitting and predicting completed\n",
      "[14:51:45] Start fitting Lvl_0_Pipe_1_Mod_1_XGB ...\n",
      "[14:51:47] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[14:51:47] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Bootstrap : Using eth0:10.233.125.199<0>\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO NET/IB : No device found.\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO NET/Socket : Using [0]eth0:10.233.125.199<0>\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Using network Socket\n",
      "NCCL version 2.11.4+cuda11.0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Setting affinity for GPU 1 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:71087:72340 [1] NCCL INFO comm 0x7fef148c4f90 rank 0 nranks 1 cudaDev 1 busId bc000 - Init COMPLETE\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:71087:72339 [0] NCCL INFO comm 0x7feea93839a0 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "25.909815757535398 xgb single fold time\n",
      "26.299865239299834 xgb single fold time\n",
      "[14:52:15] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:71087:72393 [0] NCCL INFO comm 0x7feeab9b9e20 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "[14:52:16] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Setting affinity for GPU 1 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:71087:72394 [1] NCCL INFO comm 0x7ff5c7629b60 rank 0 nranks 1 cudaDev 1 busId bc000 - Init COMPLETE\n",
      "26.109943038783967 xgb single fold time\n",
      "26.07239223923534 xgb single fold time\n",
      "[14:52:43] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:71087:72447 [0] NCCL INFO comm 0x7ff5c5f47c30 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "24.826936542987823 xgb single fold time\n",
      "[14:53:08] Time history [28.593779802322388, 28.40408444404602, 25.902260541915894]. Time left 202.22959193858225\n",
      "[14:53:14] Lvl_0_Pipe_1_Mod_1_XGB fitting and predicting completed\n",
      "[14:53:14] Time left 484.71 secs\n",
      "\n",
      "[14:53:14] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[14:53:15] Layer \u001b[1m2\u001b[0m train process start. Time left 483.86 secs\n",
      "[14:53:27] Start fitting Lvl_1_Pipe_0_Mod_0_LinearL2 ...\n",
      "[14:53:28] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "8.928589522838593e-06 getting data\n",
      "0.001032860018312931 transfering model\n",
      "4.241205292753875 fit data\n",
      "0.023167693987488747 predict data\n",
      "[14:53:33] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "9.123235940933228e-06 getting data\n",
      "0.0006183357909321785 transfering model\n",
      "4.189496451057494 fit data\n",
      "0.02079314086586237 predict data\n",
      "[14:53:38] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "9.177252650260925e-06 getting data\n",
      "0.0005960697308182716 transfering model\n",
      "4.291522792540491 fit data\n",
      "0.019257194362580776 predict data\n",
      "[14:53:43] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "8.283182978630066e-06 getting data\n",
      "0.0005746623501181602 transfering model\n",
      "5.200101841241121 fit data\n",
      "0.019586633890867233 predict data\n",
      "[14:53:49] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_1_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "1.1635944247245789e-05 getting data\n",
      "0.000764274038374424 transfering model\n",
      "4.792864042334259 fit data\n",
      "0.018093335442245007 predict data\n",
      "[14:53:54] Time history [38.57163763046265, 16.313255071640015, 1.8151881694793701, 1.8812601566314697, 1.8250467777252197, 4.266340732574463, 4.211747407913208, 4.3121819496154785, 5.2210938930511475, 4.81254243850708]. Time left 267.5466950943593\n",
      "[14:53:54] Lvl_1_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[14:53:54] Time left 444.69 secs\n",
      "\n",
      "[14:53:57] Start fitting Lvl_1_Pipe_1_Mod_0_CatBoost_gpu ...\n",
      "[14:53:59] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_1_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:53:59] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_1_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:54:12] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_1_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:54:13] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_1_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:54:25] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_1_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:54:35] Time history [12.731858253479004, 13.030681848526001, 12.995792150497437, 11.515397071838379, 13.363022327423096, 12.875388145446777, 11.919609069824219]. Time left 331.1260998249041\n",
      "[14:54:41] Lvl_1_Pipe_1_Mod_0_CatBoost_gpu fitting and predicting completed\n",
      "[14:54:41] Time left 397.51 secs\n",
      "\n",
      "[14:54:41] \u001b[1mLayer 2 training completed.\u001b[0m\n",
      "\n",
      "[14:54:41] Blending: Optimization starts with equal weights and score -0.415233314037323\n",
      "[14:54:41] Blending, iter 0: score = -0.4001850187778473, weights = [0.9442719  0.05572809]\n",
      "[14:54:41] Blending, iter 1: score = -0.4001850187778473, weights = [0.9442719  0.05572809]\n",
      "[14:54:41] No score update. Terminated\n",
      "[14:54:41] \u001b[1mAutoml preset training completed in 322.97 seconds\u001b[0m\n",
      "\n",
      "[14:54:41] Model description:\n",
      "Models on level 0:\n",
      "\t 5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2\n",
      "\t 5 averaged models Lvl_0_Pipe_1_Mod_0_CatBoost_gpu\n",
      "\t 5 averaged models Lvl_0_Pipe_1_Mod_1_XGB\n",
      "\n",
      "Final prediction for new objects (level 1) = \n",
      "\t 0.94427 * (5 averaged models Lvl_1_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.05573 * (5 averaged models Lvl_1_Pipe_1_Mod_0_CatBoost_gpu) \n",
      "\n",
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe.Custom logger is already specified. Specify more than one logger at same time is not thread safe.Custom logger is already specified. Specify more than one logger at same time is not thread safe.Custom logger is already specified. Specify more than one logger at same time is not thread safe.Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 106, in <module>\n",
      "    test_pred = automl.predict(test.reset_index(drop=True)).data\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 604, in predict\n",
      "    pred = super(TabularAutoML_gpu.__bases__[0], self).predict(data, features_names, return_all_predictions)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 298, in predict\n",
      "    level_predictions.append(ml_pipe.predict(dataset))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/ml/base.py\", line 161, in predict\n",
      "    pred = model.predict(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/base_gpu.py\", line 240, in predict\n",
      "    pred = self.predict_single_fold(model, dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/linear_gpu.py\", line 237, in predict_single_fold\n",
      "    pred = model.predict(dataset_data, dev_id)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/torch_based/linear_model_distributed.py\", line 415, in predict\n",
      "    pred = super().predict(data)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/torch_based/linear_model_distributed.py\", line 337, in predict\n",
      "    data_num, data_cat, _, _ = self._prepare_data(data)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/torch_based/linear_model_distributed.py\", line 198, in _prepare_data\n",
      "    return self._prepare_data_dense(data, y, weights, rank)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/torch_based/linear_model_distributed.py\", line 117, in _prepare_data_dense\n",
      "    data_ = torch.as_tensor(data.values.astype(cp.float32), device=device_id)\n",
      "AttributeError: 'cupy._core.core.ndarray' object has no attribute 'values'\n",
      "\n",
      "Processing airlines_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARD TIMEOUT!\n",
      "Processing jannis_0\n",
      "Processing nomao_0\n",
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset nomao, fold 0\n",
      "    V1    V2        V3        V4  ...  V116  V117      V118  Class\n",
      "0  1.0  1.00  1.000000  1.000000  ...     2   1.0  1.000000      1\n",
      "1  1.0  0.75  0.857143  0.857143  ...     2   1.0  1.000000      1\n",
      "2  1.0  1.00  1.000000  1.000000  ...     2   1.0  1.000000      1\n",
      "3  1.0  0.75  0.857143  0.857143  ...     2   1.0  0.999994      1\n",
      "4  0.0  0.00  0.250000  0.000000  ...     1   1.0  0.979322      1\n",
      "\n",
      "[5 rows x 119 columns]\n",
      "setting gpuids with all\n",
      "[15:00:27] Stdout logging level is DEBUG.\n",
      "[15:00:27] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[15:00:27] Task: binary\n",
      "\n",
      "[15:00:27] Start automl preset with listed constraints:\n",
      "[15:00:27] - time: 720.00 seconds\n",
      "[15:00:27] - CPU: 4 cores\n",
      "[15:00:27] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 4\n",
      "Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 99, in <module>\n",
      "    oof_predictions = automl.fit_predict(train.reset_index(drop=True),\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 558, in fit_predict\n",
      "    oof_pred = super(TabularAutoML_gpu.__bases__[0], self).fit_predict(train, roles=roles, cv_iter=cv_iter, valid_data=valid_data, verbose=verbose)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/base.py\", line 204, in fit_predict\n",
      "    result = super().fit_predict(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 187, in fit_predict\n",
      "    train_dataset = self.reader.fit_read(train_data, train_features, roles)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/reader/hybrid_reader.py\", line 187, in fit_read\n",
      "    output = p(delayed(call_reader)(reader, train_data[name], target=train_data[self.target]) for (reader, name) in zip(readers, names))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/concurrent/futures/_base.py\", line 444, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n",
      "    raise self._exception\n",
      "RuntimeError: CUDA error encountered at: ../src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory\n",
      "\n",
      "Processing Amazon_employee_access_0\n",
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset Amazon_employee_access, fold 0\n",
      "   RESOURCE  MGR_ID  ROLE_ROLLUP_1  ...  ROLE_FAMILY  ROLE_CODE  target\n",
      "0     39353   85475         117961  ...       290919     117908       1\n",
      "1     17183    1540         117961  ...       308574     118539       1\n",
      "2     36724   14457         118219  ...        19721     117880       1\n",
      "3     36135    5396         117961  ...       290919     118322       1\n",
      "4     42680    5905         117929  ...        19793     119325       1\n",
      "\n",
      "[5 rows x 10 columns]\n",
      "setting gpuids with all\n",
      "[15:00:40] Stdout logging level is DEBUG.\n",
      "[15:00:40] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[15:00:40] Task: binary\n",
      "\n",
      "[15:00:40] Start automl preset with listed constraints:\n",
      "[15:00:40] - time: 720.00 seconds\n",
      "[15:00:40] - CPU: 4 cores\n",
      "[15:00:40] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 4\n",
      "Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 99, in <module>\n",
      "    oof_predictions = automl.fit_predict(train.reset_index(drop=True),\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 558, in fit_predict\n",
      "    oof_pred = super(TabularAutoML_gpu.__bases__[0], self).fit_predict(train, roles=roles, cv_iter=cv_iter, valid_data=valid_data, verbose=verbose)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/base.py\", line 204, in fit_predict\n",
      "    result = super().fit_predict(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 187, in fit_predict\n",
      "    train_dataset = self.reader.fit_read(train_data, train_features, roles)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/reader/hybrid_reader.py\", line 187, in fit_read\n",
      "    output = p(delayed(call_reader)(reader, train_data[name], target=train_data[self.target]) for (reader, name) in zip(readers, names))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/concurrent/futures/_base.py\", line 444, in result\n",
      "    return self.__get_result()\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n",
      "    raise self._exception\n",
      "RuntimeError: CUDA error encountered at: ../src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory\n",
      "\n",
      "Processing robert_0\n",
      "/home/user/conda/envs/rapids-21.08/bin/python\n",
      "Train dataset airlines, fold 0\n",
      "  Airline  Flight AirportFrom AirportTo  DayOfWeek  Time  Length  Delay\n",
      "0      CO     269         SFO       IAH          3    15     205      1\n",
      "1      US    1558         PHX       CLT          3    15     222      1\n",
      "2      AA    2400         LAX       DFW          3    20     165      1\n",
      "3      AA    2466         SFO       DFW          3    20     195      1\n",
      "4      AS     108         ANC       SEA          3    30     202      0\n",
      "setting gpuids with all\n",
      "[14:55:04] Stdout logging level is DEBUG.\n",
      "[14:55:04] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[14:55:04] Task: binary\n",
      "\n",
      "[14:55:04] Start automl preset with listed constraints:\n",
      "[14:55:04] - time: 720.00 seconds\n",
      "[14:55:04] - CPU: 4 cores\n",
      "[14:55:04] - memory: 16 GB\n",
      "\n",
      "READER:\n",
      "  output is: mgpu\n",
      "  num_gpu_readers: 2\n",
      "  num_cpu_readers: 0\n",
      "[14:55:11] Train data shape: (485444, 8)\n",
      "daskcudf reader: 1.0088482107967138\n",
      "hybrid reader: 8.93781912419945\n",
      "[14:55:12] Layer \u001b[1m1\u001b[0m train process start. Time left 711.06 secs\n",
      "[14:55:33] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[14:55:36] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "cudf reader: 3.720255305059254\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "cudf reader: 3.8158803088590503\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "9.32253897190094e-06 getting data\n",
      "3.5916690919548273 transfering model\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "50.69785204343498 fit data\n",
      "0.016291609965264797 predict data\n",
      "[14:56:34] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "1.0203570127487183e-05 getting data\n",
      "0.0016159750521183014 transfering model\n",
      "6.651283346116543 fit data\n",
      "0.010844581760466099 predict data\n",
      "[14:56:45] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "9.790994226932526e-06 getting data\n",
      "0.0013537034392356873 transfering model\n",
      "6.795386272482574 fit data\n",
      "0.011284444481134415 predict data\n",
      "[14:56:55] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "8.3092600107193e-06 getting data\n",
      "0.0013451138511300087 transfering model\n",
      "3.7954487251117826 fit data\n",
      "0.013306206092238426 predict data\n",
      "[14:57:03] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "9.3923881649971e-06 getting data\n",
      "0.0015258165076375008 transfering model\n",
      "3.7786905504763126 fit data\n",
      "0.032220044173300266 predict data\n",
      "[14:57:07] Time history [54.31564521789551, 6.665184020996094, 6.8092944622039795, 3.811394691467285, 3.971001148223877]. Time left 102.75023718313733\n",
      "[14:57:07] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[14:57:07] Time left 596.66 secs\n",
      "\n",
      "[14:57:17] Start fitting Selector_CatBoost_gpu ...\n",
      "[14:57:17] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mSelector_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:57:39] Time history [22.431687116622925]. Time left 63.275892615318284\n",
      "[14:57:39] Selector_CatBoost_gpu fitting and predicting completed\n",
      "[14:57:52] Start fitting Lvl_0_Pipe_1_Mod_0_CatBoost_gpu ...\n",
      "[14:58:00] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:58:00] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:58:28] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:58:28] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:58:54] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_CatBoost_gpu\u001b[0m (par) =====\n",
      "[14:59:16] Time history [22.431687116622925, 28.309840202331543, 29.672371864318848, 25.55760097503662]. Time left 12.237217187881456\n",
      "[14:59:35] Lvl_0_Pipe_1_Mod_0_CatBoost_gpu fitting and predicting completed\n",
      "[14:59:35] Start fitting Lvl_0_Pipe_1_Mod_1_XGB ...\n",
      "[14:59:41] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "[14:59:42] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Bootstrap : Using eth0:10.233.125.199<0>\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO NET/IB : No device found.\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO NET/Socket : Using [0]eth0:10.233.125.199<0>\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Using network Socket\n",
      "NCCL version 2.11.4+cuda11.0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Setting affinity for GPU 1 to ffffff00,0000ffff,ff000000\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:73040:74509 [1] NCCL INFO comm 0x7f273e8556c0 rank 0 nranks 1 cudaDev 1 busId bc000 - Init COMPLETE\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:73040:74508 [0] NCCL INFO comm 0x7f2e03f58a10 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "27.006636234931648 xgb single fold time\n",
      "34.090875301510096 xgb single fold time\n",
      "[15:00:24] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Setting affinity for GPU 1 to ffffff00,0000ffff,ff000000\n",
      "[15:00:24] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Connected all rings\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO Connected all trees\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:73040:74596 [1] NCCL INFO comm 0x7f273e887bc0 rank 0 nranks 1 cudaDev 1 busId bc000 - Init COMPLETE\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:73040:74595 [0] NCCL INFO comm 0x7f2e043e8f40 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "32.901404776610434 xgb single fold time\n",
      "36.339983127079904 xgb single fold time\n",
      "[15:01:04] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_XGB\u001b[0m (par) =====\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 00/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 01/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 02/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 03/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 04/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 05/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 06/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 07/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 08/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 09/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 10/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 11/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 12/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 13/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 14/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 15/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 16/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 17/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 18/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 19/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 20/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 21/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 22/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 23/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 24/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 25/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 26/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 27/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 28/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 29/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 30/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Channel 31/32 :    0\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Connected all rings\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO Connected all trees\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "lama-gpu-0:73040:75954 [0] NCCL INFO comm 0x7f27bc73dc40 rank 0 nranks 1 cudaDev 0 busId 3b000 - Init COMPLETE\n",
      "30.799095598980784 xgb single fold time\n",
      "[15:01:35] Time history [41.35595107078552, 44.105671644210815, 34.71375584602356]. Time left 256.823120355606\n",
      "[15:01:54] Lvl_0_Pipe_1_Mod_1_XGB fitting and predicting completed\n",
      "[15:01:54] Time left 309.75 secs\n",
      "\n",
      "[15:01:54] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[15:01:54] Blending: Optimization starts with equal weights and score 0.7005777955055237\n",
      "[15:01:57] Blending, iter 0: score = 0.7048296928405762, weights = [0.        0.1727082 0.8272918]\n",
      "[15:01:59] Blending, iter 1: score = 0.7048296928405762, weights = [0.        0.1727082 0.8272918]\n",
      "[15:01:59] No score update. Terminated\n",
      "[15:01:59] \u001b[1mAutoml preset training completed in 415.51 seconds\u001b[0m\n",
      "\n",
      "[15:01:59] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.17271 * (5 averaged models Lvl_0_Pipe_1_Mod_0_CatBoost_gpu) +\n",
      "\t 0.82729 * (5 averaged models Lvl_0_Pipe_1_Mod_1_XGB) \n",
      "\n",
      "/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/cudf/core/frame.py:2599: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  warnings.warn(\n",
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe.Custom logger is already specified. Specify more than one logger at same time is not thread safe.Traceback (most recent call last):\n",
      "  File \"runners/lama_pf.py\", line 106, in <module>\n",
      "    test_pred = automl.predict(test.reset_index(drop=True)).data\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/presets/tabular_gpu_presets.py\", line 604, in predict\n",
      "    pred = super(TabularAutoML_gpu.__bases__[0], self).predict(data, features_names, return_all_predictions)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/automl/base.py\", line 298, in predict\n",
      "    level_predictions.append(ml_pipe.predict(dataset))\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/pipelines/ml/base.py\", line 161, in predict\n",
      "    pred = model.predict(dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/base_gpu.py\", line 240, in predict\n",
      "    pred = self.predict_single_fold(model, dataset)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/boost_cb_gpu.py\", line 345, in predict_single_fold\n",
      "    pred = self._predict(model, cb_test, params)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/lightautoml/ml_algo/boost_cb_gpu.py\", line 388, in _predict\n",
      "    pred = model.predict(\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/catboost/core.py\", line 2324, in predict\n",
      "    return self._predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, 'predict', task_type)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/catboost/core.py\", line 2272, in _predict\n",
      "    predictions = self._base_predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n",
      "  File \"/home/user/conda/envs/rapids-21.08/lib/python3.8/site-packages/catboost/core.py\", line 1568, in _base_predict\n",
      "    return self._object._base_predict(pool, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n",
      "  File \"_catboost.pyx\", line 4538, in _catboost._CatBoost._base_predict\n",
      "  File \"_catboost.pyx\", line 4545, in _catboost._CatBoost._base_predict\n",
      "_catboost.CatBoostError: catboost/libs/data/model_dataset_compatibility.cpp:81: At position 0 should be feature with name le_gpu__Airline (found Length).\n",
      "\n",
      "Processing aps_failure_0\n"
     ]
    }
   ],
   "source": [
    "Parallel(n_jobs=len(GPU_PARTS), backend=\"threading\")(\n",
    "    delayed(run_task)(benchmark_path, data_path, d, f, False) for (d, f) in product(data_info, range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f8bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_lama_venv",
   "language": "python",
   "name": "new_lama_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

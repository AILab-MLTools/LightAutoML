{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import usual libraries\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = 'gpu'\n",
    "\n",
    "import cudf\n",
    "\n",
    "# import lightautoml\n",
    "from lightautoml.automl.presets.text_presets import TabularNLPAutoML\n",
    "from lightautoml.automl.presets.gpu.text_gpu_presets import TabularNLPAutoMLGPU\n",
    "from lightautoml.tasks import Task\n",
    "from lightautoml.dataset.utils import roles_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nlp constants\n",
    "N_THREADS = 4\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "TIMEOUT = 300\n",
    "TARGET_NAME = 'is_good'\n",
    "\n",
    "torch.set_num_threads(N_THREADS)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bankiru dataset\n",
    "DATASET_FULLNAME = 'data/bankiru_isgood.csv'\n",
    "\n",
    "# here only 1000 samples are used for time reasons (for a detailed check, one needs to use larger number:\n",
    "# 100k-500k)\n",
    "data = pd.read_csv(DATASET_FULLNAME)[[\"message\", \"title\", \"is_good\"]].fillna(\"\")[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "tr_data, te_data = train_test_split(data,\n",
    "        test_size=TEST_SIZE,\n",
    "        stratify=data[TARGET_NAME],\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "print(data.head())\n",
    "tr_data = pd.DataFrame(data, index=[i for i in range(tr_data.shape[0])])\n",
    "te_data = pd.DataFrame(data, index=[i for i in range(te_data.shape[0])])\n",
    "\n",
    "print(f'Data splitted. Parts sizes: tr_data = {tr_data.shape}, te_data = {te_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define task and roles\n",
    "task = Task('binary', device=device)\n",
    "\n",
    "roles = {\n",
    "    'text': ['message', 'title'],\n",
    "    'target': TARGET_NAME,\n",
    "}\n",
    "print(roles_parser(roles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_automl(automl, tr_data, te_data):\n",
    "    t0 = time.time()\n",
    "    oof_pred = automl.fit_predict(tr_data, roles=roles, verbose=1)\n",
    "    t1 = time.time()\n",
    "    print('Elapsed time (train): {}'.format(t1 - t0))\n",
    "\n",
    "    t0 = time.time()\n",
    "    te_pred = automl.predict(te_data)\n",
    "    t1 = time.time()\n",
    "    print('Elapsed time (test): {}'.format(t1 - t0))\n",
    "\n",
    "    not_nan = np.any(~np.isnan(oof_pred.data), axis=1)\n",
    "    print(f'OOF score: {roc_auc_score(tr_data[TARGET_NAME].values[not_nan], oof_pred.data[not_nan][:, 0])}')\n",
    "    print(f'TEST score: {roc_auc_score(te_data[TARGET_NAME].values, te_pred.data[:, 0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear_l2 model with different text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "n_oversample = 0\n",
    "ngram = (1, 1)\n",
    "\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "            timeout=600,\n",
    "            cpu_limit=1,\n",
    "            gpu_ids='0',\n",
    "            client=None,\n",
    "            general_params={\n",
    "                'nested_cv': False,\n",
    "                'use_algos': [['linear_l2']]\n",
    "            },\n",
    "            reader_params={\n",
    "                'npartitions': 2\n",
    "            },\n",
    "            text_params={\n",
    "                'lang': 'ru',\n",
    "                'verbose': False,\n",
    "                'use_stem': False,\n",
    "            },\n",
    "            tfidf_params={\n",
    "                'n_components': n_components,\n",
    "                'n_oversample': n_oversample,\n",
    "                'tfidf_params': {'ngram_range': ngram}\n",
    "            },\n",
    "            linear_pipeline_params={\n",
    "                'text_features': \"tfidf\"\n",
    "            }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### tfidf_subword features\n",
    "\n",
    "The following __text_params__ work only with __tfidf_subword__ text features:   \n",
    "__vocab_path__ - path to vocabulary .txt file,  \n",
    "__data_path__ - .txt file (saved pd.Series) for the tokenizer to be trained on (if vocab is not specified)  \n",
    "__is_hash__ - True means vocab is not raw vocab but was transformed with hash_vocab function from cudf,  \n",
    "__max_length__ - max number of tokens to leave in one text (exceeding ones would be truncated)  \n",
    "__tokenizer__ - [\"bpe\" or \"wordpiece\"] if vocab is None. Type of tokenizer to be trained  \n",
    "__vocab_size__ - vocabulary size for trained tokenizer  \n",
    "__save_path__ - path where trained vocabulary would be saved to  \n",
    "\n",
    "Overall, there are 3 possible scenarios to run tfidf_subword text features:  \n",
    "1) __vocab_path__ is defined, __is_hash__ = True. It means that __vocab_path__ contains path to a hashed version of vocabulary. No additional transformation is needed. This is the optimal usage (all vocabulary pre-processing was done in advance).\n",
    "2) __vocab_path__ is defined, __is_hash__ = False. __vocab_path__ contains path to a vocabulary with raw words, it needs to be transformed to a hash version. This is the second fastest option.\n",
    "3) __vocab_path__ is not defined, __data_path__ is defined (with additional parameters __tokenizer__, __vocab_size__ and __save_path__). Only .txt file of a dataframe is available. Note, that it works not with a dataframe itself but with its .txt version. One should be careful with tokenizer settings. Recommended way is to study the dataset in advance, tweak tokenizer settings and create the vocabulary aside from LAMA pipeline. The quality of __tfidf_subword__ text features highly depend on the quality of the used tokenizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for all scenarios. Imagine that only pd.Series of text data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: how to create .txt dataframe (one should save only text corpus)\n",
    "# This is an example, it is not necessary to run it\n",
    "\n",
    "# Step 1. Choose your representative text data and save it to .txt file. Here only one column of text dataset \n",
    "# is taken but sometimes it might be a good idea to concatenate all text columns instead of choosing one.\n",
    "data_text = data['message']\n",
    "file_data_text = 'bankiru_isgood_test.txt'\n",
    "with open(file_data_text, 'w+') as f:\n",
    "    for i in range(len(data_text)):\n",
    "        f.write(data_text.iloc[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: how to use huggingface tokenizer to create vocabulary from .txt dataframe\n",
    "# This is an example, it is not necessary to run it\n",
    "\n",
    "# Step 2. Having a text data file, train token vocabulary.\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "\n",
    "tokenizer = 'bpe' # or 'wordpiece'\n",
    "vocab_size = 30000\n",
    "data_path = file_data_text # path to a .txt pd.Series of text data\n",
    "vocab_save_path = f\"{tokenizer}_{vocab_size // 1000}k_test.txt\"\n",
    "\n",
    "if tokenizer == \"bpe\":\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[SEP]\", \"[CLS]\"]\n",
    "    )\n",
    "else:\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[SEP]\", \"[CLS]\"]\n",
    "    )\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer.train([data_path], trainer) # train tokenizer on out .txt text data\n",
    "trained_vocab = tokenizer.get_vocab()\n",
    "\n",
    "# save trained vocabulary to a .txt file\n",
    "with open(vocab_save_path, 'w+') as f:\n",
    "    for key in trained_vocab.keys():\n",
    "        f.write(key + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: how to create hash vocabulary from word .txt vocabulary\n",
    "# This is an example, it is not necessary to run it\n",
    "\n",
    "# Step 3. Having .txt vocabulary file, create a hashed version of it which would be used by \n",
    "# cudf.SubwordTokenizer\n",
    "from cudf.utils.hash_vocab_utils import hash_vocab\n",
    "\n",
    "vocab_save_path_hash = vocab_save_path.split('.')[0]+'_hash.txt'\n",
    "hash_vocab(vocab_save_path, vocab_save_path_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Step 1-2. Download existing vocabulary (one could use data from huggingfsce models).\n",
    "\n",
    "# Download standard bert English vocabulary\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
    "bert_vocab_en_path = 'bert-base-uncased-vocab.txt'\n",
    "# Download bert Russian vocabulary\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased/vocab.txt\n",
    "bert_vocab_ru_path = 'vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True to use data generated in this notebook, False to use data available in zip dataset archive\n",
    "use_test_data = True \n",
    "\n",
    "if use_test_data:\n",
    "    bankiru_info = {'path': 'data/bankiru_isgood.csv',\n",
    "                    'text_roles': ['message', 'title'],\n",
    "                    'target': 'is_good',\n",
    "                    'task': 'binary',\n",
    "                    'lang': 'ru',\n",
    "                    'csv2text': file_data_text,\n",
    "                    'vocab_path': vocab_save_path,\n",
    "                    'vocab_hash_path': vocab_save_path_hash\n",
    "    }\n",
    "else:\n",
    "    bankiru_info = {'path': 'data/bankiru_isgood.csv',\n",
    "                    'text_roles': ['message', 'title'],\n",
    "                    'target': 'is_good',\n",
    "                    'task': 'binary',\n",
    "                    'lang': 'ru',\n",
    "                    'csv2text': 'data/csv2text/bankiru_isgood.txt',\n",
    "                    'vocab_path': 'data/vocab/bankiru_isgood_vocab.txt',\n",
    "                    'vocab_hash_path': 'data/vocab_hash/bankiru_isgood_vocab_hash.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario 1\n",
    "automl = TabularNLPAutoMLGPU(task=task, \n",
    "                              timeout=600, \n",
    "                              cpu_limit=1, \n",
    "                              gpu_ids='0', \n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False,\n",
    "                                  'vocab_path': bankiru_info['vocab_hash_path'],\n",
    "                                  'is_hash': True,\n",
    "                                  # 'data_path': file_name,\n",
    "                                  # 'tokenizer': \"bpe\",\n",
    "                                  # 'vocab_size': 30000\n",
    "                              },\n",
    "                              tfidf_params={\n",
    "                                  'n_components': n_components,\n",
    "                                  'n_oversample': n_oversample,\n",
    "                                  'tfidf_params': {'ngram_range': ngram}\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'tfidf_subword'\n",
    "                              },\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario 2\n",
    "automl = TabularNLPAutoMLGPU(task=task, \n",
    "                              timeout=600, \n",
    "                              cpu_limit=1, \n",
    "                              gpu_ids='0', \n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False,\n",
    "                                  'vocab_path': bankiru_info['vocab_path'],\n",
    "                                  'is_hash': False,\n",
    "                                  # 'data_path': file_name,\n",
    "                                  # 'tokenizer': \"bpe\",\n",
    "                                  # 'vocab_size': 30000\n",
    "                              },\n",
    "                              tfidf_params={\n",
    "                                  'n_components': n_components,\n",
    "                                  'n_oversample': n_oversample,\n",
    "                                  'tfidf_params': {'ngram_range': ngram}\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'tfidf_subword'\n",
    "                              },\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario 3\n",
    "automl = TabularNLPAutoMLGPU(task=task, \n",
    "                              timeout=600, \n",
    "                              cpu_limit=1, \n",
    "                              gpu_ids='0', \n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False,\n",
    "                                  'vocab_path': None,\n",
    "                                  'data_path': bankiru_info['csv2text'],\n",
    "                                  'tokenizer': \"bpe\",\n",
    "                                  'vocab_size': 30000\n",
    "                              },\n",
    "                              tfidf_params={\n",
    "                                  'n_components': n_components,\n",
    "                                  'n_oversample': n_oversample,\n",
    "                                  'tfidf_params': {'ngram_range': ngram}\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'tfidf_subword'\n",
    "                              },\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### embed text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One should note that gensim package was removed, now only torchnlp embeddings are available of fixed\n",
    "# dimensionality\n",
    "model_name = 'random_lstm'\n",
    "\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "                              timeout=600,\n",
    "                              cpu_limit=1,\n",
    "                              gpu_ids='0',\n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False\n",
    "                              },\n",
    "                              autonlp_params={\n",
    "                                  'model_name': model_name,\n",
    "                                  'sent_scaler': 'l1',\n",
    "                                  'embedding_model': 'fasttext', # now this has a different meaning\n",
    "                                  'cache_dir': None\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'embed'\n",
    "                              },\n",
    "                              )\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'borep'\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "                              timeout=600,\n",
    "                              cpu_limit=1,\n",
    "                              gpu_ids='0',\n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False\n",
    "                              },\n",
    "                              autonlp_params={\n",
    "                                  'model_name': model_name,\n",
    "                                  'sent_scaler': None,\n",
    "                                  'embedding_model': 'fasttext', # now this has a different meaning\n",
    "                                  'cache_dir': None\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'embed'\n",
    "                              },\n",
    "                              )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'random_lstm_bert'\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "                              timeout=600,\n",
    "                              cpu_limit=1,\n",
    "                              gpu_ids='0',\n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False\n",
    "                              },\n",
    "                              autonlp_params={\n",
    "                                  'model_name': model_name,\n",
    "                                  'sent_scaler': 'l2',\n",
    "                                  'embedding_model': 'fasttext', # now this has a different meaning\n",
    "                                  'cache_dir': None\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'embed'\n",
    "                              },\n",
    "                              )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pooled_bert'\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "                              timeout=600,\n",
    "                              cpu_limit=1,\n",
    "                              gpu_ids='0',\n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False\n",
    "                              },\n",
    "                              autonlp_params={\n",
    "                                  'model_name': model_name,\n",
    "                                  'sent_scaler': 'l2',\n",
    "                                  'embedding_model': 'fasttext', # now this has a different meaning\n",
    "                                  'cache_dir': None\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'embed'\n",
    "                              },\n",
    "                              )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'wat'\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "                              timeout=600,\n",
    "                              cpu_limit=1,\n",
    "                              gpu_ids='0',\n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False\n",
    "                              },\n",
    "                              autonlp_params={\n",
    "                                  'model_name': model_name,\n",
    "                                  'sent_scaler': None,\n",
    "                                  'embedding_model': 'fasttext', # now this has a different meaning\n",
    "                                  'cache_dir': None\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'embed'\n",
    "                              },\n",
    "                              )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost and xgb algos with tfidf text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "n_components = 100\n",
    "n_oversample = 0\n",
    "ngram = (1, 1)\n",
    "\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "            timeout=600,\n",
    "            cpu_limit=1,\n",
    "            gpu_ids='0',\n",
    "            client=None,\n",
    "            general_params={\n",
    "                'nested_cv': False,\n",
    "                'use_algos': [['cb']]\n",
    "            },\n",
    "            reader_params={\n",
    "                'npartitions': 2\n",
    "            },\n",
    "            text_params={\n",
    "                'lang': 'ru',\n",
    "                'verbose': False,\n",
    "                'use_stem': False,\n",
    "            },\n",
    "            tfidf_params={\n",
    "                'n_components': n_components,\n",
    "                'n_oversample': n_oversample,\n",
    "                'tfidf_params': {'ngram_range': ngram}\n",
    "            },\n",
    "            linear_pipeline_params={\n",
    "                'text_features': \"tfidf\"\n",
    "            }\n",
    "            )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "n_components = 100\n",
    "n_oversample = 0\n",
    "ngram = (1, 1)\n",
    "\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "            timeout=600,\n",
    "            cpu_limit=1,\n",
    "            gpu_ids='0',\n",
    "            client=None,\n",
    "            general_params={\n",
    "                'nested_cv': False,\n",
    "                'use_algos': [['xgb']]\n",
    "            },\n",
    "            reader_params={\n",
    "                'npartitions': 2\n",
    "            },\n",
    "            text_params={\n",
    "                'lang': 'ru',\n",
    "                'verbose': False,\n",
    "                'use_stem': False,\n",
    "            },\n",
    "            tfidf_params={\n",
    "                'n_components': n_components,\n",
    "                'n_oversample': n_oversample,\n",
    "                'tfidf_params': {'ngram_range': ngram}\n",
    "            },\n",
    "            linear_pipeline_params={\n",
    "                'text_features': \"tfidf\"\n",
    "            }\n",
    "            )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
